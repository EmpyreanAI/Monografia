Neste capítulo, apresenta-se os conceitos básicos de economia e mercado, discorrendo sobre o mercado de ações e a teoria moderna de portfólio. Em seguidas discutem-se conceitos relacionados a área de aprendizado de máquina, apresentando juntamente algoritmos relevantes para este trabalho. Posteriormente é apresentado o conceito de aprendizado profundo e seus algoritmos utilizados. Por fim, discorre-se sobre o aprendizado por reforço com técnicas de aprendizado profundo e o algoritmo de Gradiente de Política Determinística Profunda.


\section{Economia e Mercado}

A riqueza material de uma sociedade é determinada pela capacidade produtiva de sua economia, representada pelos bens e serviços que esta fornece aos membros da mesma \cite{investments}. Esta capacidade produtiva, ou produtividade, é definida diretamente pelos chamados ativos reais, tais como: construções, conhecimento, terras, máquinas e qualquer elemento da sociedade que são necessários para gerar esses bens \cite{investments}.

Em contraste aos ativos reais existe os ativos financeiros. Esses ativos não possuem a capacidade de representar a riqueza de uma sociedade, contudo conseguem afetar a produtividade de maneira indireta \cite{investments}. Ações e títulos são os exemplos mais simples de ativos financeiros. Estes conseguem definir os donos de propriedades e frações de empresas, gerando assim uma facilidade na transação de recursos entre os proprietários de um ativo real \cite{investments}.

Em suma, ativos reais produzem bens e serviços, enquanto ativos financeiros definem alocação de riqueza e lucro entre investidores \cite{investments}. No entanto, ativos financeiros representam posses de frações de ativos reais. Com exemplo, quando o dinheiro que é investido em uma firma, ele é convertido em ativos reais. Quando tal investimento oferecer um retorno, este pode ser retribuído em ativos financeiros através de títulos que representam parte da empresa. Estes, consequentemente, representam frações de ativos reais. Portanto, ativos financeiros e os mercados nos quais são negociados preenchem um papel importante no desenvolvimento da economia \cite{investments}.

\subsection{O Mercado de Ações}

Assim como as instituições financeiras e seguradoras surgiram naturalmente a partir das necessidades de investidores, o mercado financeiro não é diferente \cite{investments}. A necessidade de uma instituição para unir, em um mesmo lugar, interessados em negociar ativos financeiros fez surgir os mercados de ações e a bolsas de valores. \cite{investments}.

Chama-se de mercado toda interação entre dois ou mais interessados que se identifiquem como compradores e vendedores. Dentre os tipos de mercado podemos dividi-los em quatro categorias: (i) o mercado de pesquisa direta, em que os compradores e vendedores precisam se procurar por conta própria; (ii) o mercado intermediado onde existe um intermediador, entre o comprador e o vendedor, que obterá algum lucro na negociação; (iii) o mercado de revendedores em que o intermediador compra os ativos e revende para os vendedores; e por último (iv) o mercado de leilão onde todos os transacionistas se reúnem em um mesmo local para fazerem ofertas \cite{investments}. O mercado de leilão é o melhor exemplo de mercado financeiro moderno, criando a vantagem de reunir todas as ofertas em um mesmo lugar sem a necessidade de pesquisas por preços mais baratos \cite{investments}.

Dentre os mercados de leilão, existem os mercados de ações, comumente chamados de bolsa de valores. Em geral, uma bolsa de valores é um mercado onde são efetuadas ações de compra e venda de valores mobiliários (ações, títulos, derivativos, e entre outros) \cite{gomes1997bolsa}. Tais valores podem ser vistos como contratos legais que representam o direito de receber um benefício futuro \cite{elton2012moderna}. Nesse trabalho destacaremos dois principais valores mobiliários: ações preferenciais e ações ordinárias \cite{elton2012moderna}. Ações ordinárias representam o direito de um investidor sobre os rendimentos ou ativos de uma corporação, possuindo como principal característica a responsabilidade limitada de seus detentores \cite{investments}. Se a corporação for a falência o máximo que o investidor de ações ordinárias pode perder é o seu investimento inicial, não tendo acesso aos ativos gerais da empresa. Já as ações preferenciais são semelhantes com as ações ordinários exceto pela sua prioridade nos pagamentos dos dividendos da empresa. Caso a empresa vá a falência todos os acionistas com ações preferenciais devem receber os dividendos antes dos pagamentos de dividendos de ações ordinárias \cite{elton2012moderna}.

As ações, tanto ordinárias como preferencias, tem suas cotas negociadas na bolsa de valores seguindo uma mecânica bem definida. Um indivíduo que deseja comprar ou vender um ação necessita antes contratar uma corretora, uma entidade que possui permissão para emitir ordens em uma bolsa de valores. Uma ordem é responsável por carregar as informações de compra ou venda uma ação. Esta comumente deve conter: nome do negociante e qual ativo deseja negociar, se é uma compra ou uma venda, a quantidade de ativos a serem negociados e o prazo que a ordem permanecerá válida. Assim que a ordem é emitida e executada a corretora é responsável por transacionar o dinheiro entre o indivíduo e a bolsa de valores \cite{elton2012moderna}.

\subsection{Teoria Moderna do Portfólio}

Um indivíduo que possui riquezas, pode ter seus bens referidos como o conjunto de ativos que este possui, seja eles reais ou financeiros. Esse conjunto também pode ser chamado de portfólio ou carteira~\cite{elton2012moderna}. A composição destes portfólios são resultados de decisões casuais ou podem ser fim de um planejamento pré-estabelecido \cite{elton2012moderna}.

A criação de portfólios planejados com premissa nos conceitos de otimização e diversificação da carteira, foram pilares no desenvolvimento e entendimento dos mercados financeiros \cite{kolm201460}. Em 1952 \textcite{markowitz}, publicou o artigo \emph{Portfolio Selection} que viria a ser um dos maiores avanços na criação de portfólios \cite{kolm201460}. Sua proposta ficou conhecida como a Teoria Moderna do Portfólio, ou \acrfull{VMM} \cite{markowitz}. A teoria buscava responder a questão fundamental de como um investidor deve alocar seus fundos dentre as diversas possibilidades de escolha de investimentos. 

Markowitz adereçou essa questão com duas etapas. Primeiro ele quantificou o retorno e risco de um valor mobiliário \cite{kolm201460}. O retorno foi quantificado de acordo com a medida estatística de retorno esperado demonstrada na \refEq{retorno}, onde $R_p$ é o retorno esperado do portfólio, $R_i$ o retorno do ativo $i$, e $w_i$ a proporção do ativo $i$ na carteira \cite{elton2012moderna}. Enquanto o risco foi medido pelo desvio padrão da carteira, demonstrado na \refEq{desviop}, onde $\sigma_{p}$ é o desvio padrão do retorno da carteira, $\sigma _{i}^{2}$ a variância do retorno do ativo $i$, $\sigma _{j}^{2}$ a variância do retorno do ativo $j$, e $\rho _{ij}$ o coeficiente de correlação entre os ativos $i$ e $j$ \cite{elton2012moderna}. Na segunda etapa, Markowitz sugere que os investidores devem considerar retorno e risco de forma conjunta, determinando a alocação dos fundos com base no balanceamento entre os fatores \cite{kolm201460}.

\equacao{retorno}{\operatorname {E} (R_{p})=\sum _{i}w_{i}\operatorname {E} (R_{i})\quad} 

\equacao{desviop}{\displaystyle \sigma _{p}=\sqrt{\sum _{i}w_{i}^{2}\sigma _{i}^{2}+\sum _{i}\sum _{j\neq i}w_{i}w_{j}\sigma _{i}\sigma _{j}\rho _{ij}}}

Esta ideia de que a tomada de decisão financeira sólida é realizada como um balanceamento entre retorno e risco foi revolucionária \cite{kolm201460}. Seu principio mais inovador foi o da diversificação do portfólio \cite{kolm201460}. Com este principio, o risco de um portfólio pode passar a ser analisado de acordo com a correlação dos seus constituintes, e não apenas o risco do ativo independentemente. Este conceito era estranho à análise financeira clássica, que girava em torno da crença de que os investidores deveriam investir naqueles ativos que oferecem o maior valor futuro, dado seu preço atual \cite{kolm201460}. A ideia de Markowitz, foi inovadora também por passar a formular o problema de tomadas de decisões financeiras, em um problema de otimização \cite{kolm201460}. Em particular, o modelo de otimização da \acrshort{VMM} propõe que entre os números infinitos de portfólio que alcançam um retorno pré-definido, o investidor deve escolher aquele que tem o menor risco.

\section{Aprendizado de Máquina} 

Durante milhares de anos os seres humanos vem procurando desvendar como funcionam seus pensamentos, como um punhado de matéria pode perceber, compreender, prever e manipular um mundo muito maior que ele mesmo \cite{modern_approach}. O campo da \acrfull{IA} estende o desejo de desvendar e entender o pensamento humano, indo mais além, tentando também construir entidades inteligentes \cite{modern_approach}. O desejo de construir tais entidades capazes de pensar vem desde a Grécia antiga, que apresenta diversos inventores e vidas artificiais em sua mitologia \cite{deep_learning}. Até o momento de escrita deste trabalho a \acrshort{IA} vem se mostrando uma subárea da ciência da computação próspera, com diversas aplicações práticas e tópicos de pesquisa \cite{deep_learning}. Algumas de suas aplicações são realizações de tarefas como: jogos de xadrez, demonstração de teoremas matemáticos, criação de poesia, controle robótico, e diagnóstico de doenças \cite{modern_approach}.

Pode-se definir a \acrshort{IA} de diversas formas: em termos de processos de pensamento e raciocínio, comportamento, fidelidade ao desempenho humano, ou racionalidade \cite{modern_approach}. Para o escopo deste trabalho, escolheu-se a definição de \acrshort{IA} como o processo de agir com racionalidade, definido por \textcite{poole1998computational}, ``Inteligência computacional é o estudo do projeto de agentes inteligentes''. Segundo \textcite{modern_approach}, um agente é tudo que pode ser considerado capaz de perceber seu ambiente por meio de sensores e de agir sobre esse ambiente por meio de atuadores. Para um agente humano, os olhos, ouvidos e outros sentidos são os sensores que percebem o mundo, já suas pernas, mãos bocas e outras partes do corpo são atuadores que interagem com ele. Um agente é dito inteligente, ou racional, quando toma as melhores ações para maximizar alguma medida de desempenho pré-definida para o problema sendo tratado \cite{modern_approach}. Como exemplo, considere um agente investindo na bolsa de valores, sua medida de desempenho pode ser considerada o lucro ganho com seus investimentos. Este agente é considerado racional caso a ação realizada possa vir a maximizar o seu lucro.

Em diversos cenários, como o exemplo citado, um comportamento considerado bom em determinado momento pode deixar de sê-lo em um outro contexto ou situação. Deste modo, um agente racional necessita aprender a se adaptar a situações adversas \cite{modern_approach}. Dizemos que o agente aprende, quando este melhora o seu desempenho nas tarefas futuras após observar informações atuais disponíveis a ele \cite{modern_approach}. O aprendizado apresenta outras duas vantagens além do motivo já citado do agente se adaptar a situações do ambiente, estas sendo: (i) um programador não consegue antecipar todas as situações possíveis que um agente pode encontrar; e (ii) as vezes o programador não tem ideia de como resolver o problema, e deixa a tarefa de descoberta para o agente \cite{modern_approach}.

%  como subárea da inteligencia artificial,
Neste contexto, os algoritmos de aprendizado de máquina surgem como um meio para auxiliar o agente a adquirir seu próprio conhecimento e aprender \cite{machinelearning}. A introdução do aprendizado de máquina permitiu computadores abordarem problemas envolvendo conhecimento do mundo real e tomar decisões que pareçam subjetivas \cite{deep_learning}. De maneira direta, aprender é o processo de converter experiencias em conhecimento. A entrada de um algoritmo de aprendizado são dados de treinamento, representando a experiência, e a saída é um conhecimento sobre a tarefa que está sendo aprendida \cite{shalev2014understanding}.

%Professor, posso exemplificar cada paradigma, mas ao meu ver não acho necessario
Em aprendizagem de máquina, existem diversos tipos de tarefas para a entidade aprendiz. Estas tarefas podem ser agrupadas de acordo com o paradigma de aprendizado para lidar com ela, podendo ser preditiva ou descritiva \cite{faceli2011inteligencia}. Uma tarefa preditiva busca encontrar uma função que faz o mapeamento dos dados de entrada para obter uma predição como saída \cite{modern_approach}. Algoritmos que seguem o modelo preditivo são do paradigma de \textbf{aprendizado supervisionado}. O termo supervisionado, se relaciona com a necessidade de existir um ``supervisor externo'', que conhece a resposta da tarefa (rótulo), para analisar o desempenho do aprendiz \cite{faceli2011inteligencia}. Por outro lado, em uma tarefa descritiva o objetivo é explorar ou descrever um conjunto de dados. Algoritmos que se encaixam neste tipo de classificação de tarefa são do paradigma de \textbf{aprendizado não supervisionado} \cite{faceli2011inteligencia}. Estes algoritmo não são fornecidos um rótulo \cite{modern_approach}, e portanto, não necessitam de uma supervisão externa. Outro paradigma de aprendizado, que não se encaixa nos modelos de tarefas anteriores é o de \textbf{aprendizado por reforço}. Neste paradigma a meta é reforçar ou recompensar de forma positiva ou negativa uma ação realizada pela entidade que esta aprendendo \cite{faceli2011inteligencia}. Este tipo de aprendizado será apresentado neste trabalho em detalhes na Subseção \ref{sec:rl}.

Tem-se como foco do trabalho resolver tarefas de predição, logo este discorre majoritariamente sobre o aprendizado supervisionado. Um algoritmo supervisionado é uma função que dado um conjunto de dados de entrada rotulado, constrói um estimador~\cite{faceli2011inteligencia}. Os rótulos tomam valores em um domínio conhecido. Se este domínio conter valores nominais, subclassificamos essa tarefa como \textbf{classificação}~ \cite{modern_approach}. Caso o contrário, os rótulos possuírem um conjunto infinito e ordenado de valores tem-se um problema de \textbf{regressão}~ \cite{faceli2011inteligencia}. Com um exemplo prático, uma predição na bolsa de valores pode se tomar de ambas as formas. Se deseja-se descobrir se uma determinada ação vai ganhar ou perder valor, temos um problema de classificação. Se deseja descobrir uma estimativa do preço de uma ação, o problema toma a forma de regressão, porque o domínio do valor da ação é de qualquer valor real positivo. Com o objetivo de realizar uma tarefa de predição, diversos algoritmos de aprendizado supervisionado foram criados. Para este trabalho, foram considerado os algoritmos \acrlong{KNN} e \acrlong{SVM} que são discutidos nas Subseções \ref{sec:knn} e \ref{sec:svm}.


%-Inicia da pag. 96 do livro: Discovering knowledge in data: An introduction data mining
%-Dar uma lida no cap. 8 do livro: Machine Learning, Tom M. Mitchell
%-Dar uma lida no cap. 4 do livro: Inteligência Artificial: uma abordagem de aprendizado de máquina. Esse livro em pt e é do pesquisador mais famoso atualmente na área de M.L.
\subsection{k-Vizinhos Mais Próximos}
\label{sec:knn}

No aprendizado supervisionado, a simplicidade do \acrfull{KNN} \cite{knn} se destaca entre os modelos, sendo um dos algoritmos mais usados no problema de classificação \cite{larose2014discovering}. O \acrshort{KNN} classifica objetos desconhecidos com base na similaridade com outros objetos já conhecidos \cite{larose2014discovering}. Tais objetos são definidos pelos seus atributos e representados como pontos em um espaço, onde suas similaridades são comumente definidas como a distância euclidiana entre esses pontos \cite{larose2014discovering}. Portanto, quando o algoritmo receber um ponto $x_t$ não classificado, ele calcula a distância entre $x_t$ e todos os outros pontos presentes no espaço. O rótulo selecionado para $x_t$ será o mesmo do objeto $x_i$ onde $x_i$ possui a menor distância euclidiana até $x_t$ \cite{faceli2011inteligencia}.

O \acrshort{KNN} não precisa necessariamente definir a similaridade com a distância euclidiana~\cite{larose2014discovering}. A função que define uma similaridade é conhecida como métrica de distância e é dada por uma função real $d$ de modo que para qualquer coordenadas $x$, $y$ e $z$ as propriedades definidas pela \refEq{prop1}, \refEq{prop2} e \refEq{prop3} se satisfazem \cite{}.

\equacao{prop1}{d(x,y) >= 0\\d(x,y) = 0 \iff x = y}
\equacao{prop2}{d(x,y) = d(y,x)}
\equacao{prop3}{d(x,z) <= d(x,y) + d(y,z)}

A propriedade representada pela \refEq{prop1} garante que a distância sempre será não negativa e que a única maneira de ser zero é caso as coordenadas sejam as mesmas. Já a propriedade da \refEq{prop2} indica que  distância de $x$ para $y$ deve ser a mesma de $y$ para $x$. E por último a propriedade definida pela \refEq{prop3} representa a desigualdade triangular, onde introduzir um terceiro ponto não pode reduzir a distancia entre outros dois pontos.

Porém o \acrshort{KNN} pode considerar mais de um único vizinho em suas classificações, gerando uma votação entre os vizinhos mais próximos para decidir o rótulo da entrada teste \cite{faceli2011inteligencia}. Considere a \refFig{knn} como exemplo, dado um valor de entrada $x_t$ o algoritmo seleciona os $k$ vizinhos (no exemplo, $k=5$)  mais próximos de $x_t$, assim cada vizinho vota em uma classe e $x_t$ é classificado segundo a classe mais votada. Tal votação pode ser formalmente definida pela \refEq{voto1}, onde a função de moda retorna o elemento que mais se repete, representando a classe mais votada.

\figura[h]{img/knn.pdf}{Classificação do elemento $x_t$ através do algoritmo \acrshort{KNN}}{knn}{width=.8\linewidth}%

O método de votação para classificar uma entrada não se restringe somente a moda, o \acrshort{KNN} também pode ser usado em problemas de regressão, o
que trás duas outras maneiras de se abordar a classe mais votada, em relação a função de perda que desejamos minimizar \cite{faceli2011inteligencia}. Caso a função a ser minimizada seja o erro quadrático o valor do rótulo de $x_t$ torna-se a média dos $k$ vizinhos mais próximos, formalmente dado pela \refEq{voto2} \cite{larose2014discovering}. Para o caso de que a função a ser minimizada é o desvio padrão utiliza-se a mediana como métrica de votação, sendo formalizada por \refEq{voto3}.

% Encontrar símbolos usados na estatística para moda, média e mediana
\equacao{voto1}{f'(x_t) = moda(f(x_1), f(x_2),..., f(x_k))}
\equacao{voto2}{f'(x_t) = média(f(x_1), f(x_2), \ldots, f(x_k))}
\equacao{voto3}{f'(x_t) = mediana(f(x_1), f(x_2), \ldots, f(x_k))}

\subsection{Máquina de Vetores de Suporte}
\label{sec:svm}

%oba baum? naum
Uma \acrfull{SVM} \cite{boser1992training} é um modelo de aprendizado supervisionado que mapeia pontos de um espaço de entrada, via uma função $\phi$, para um espaço de características. Ele realiza o aprendizado construindo um plano para separar classes de dados e realizar a tomada de decisões, denominado hiperplano separador \cite{scholkopf2001learning}. A ideia central, é separar classes de dados utilizando o hiperplano separador, de tal forma que os elementos de classes opostas, mais próximos entre si, fiquem o mais distante possível desse plano \cite{hamel2011knowledge}.

Dependo da dimensão dos dados de entrada o hiperplano irá assumir uma dimensão a menos do que a dimensão dos dados. Como por exemplo, em dados com duas dimensões o hiperplano separador terá uma única dimensão, sendo representado por uma linha. A lógica se mantém para dimensões superiores, se o espaço for tridimensional o hiperplano separador será um plano de duas dimensões \cite{hamel2011knowledge}. Porém nem sempre o problema apresenta dados linearmente separáveis sendo necessário mapear o espaço de entrada para dimensões superiores afim de encontrar melhores hiperplanos.

Na figura \refFig{svm}, é mostrado o processo de encontro do hiperplano ótimo para uma entrada de dados que não é linearmente separável. Os dados são mapeados para um espaço de características de maior dimensão através da função $z = x^2 + y^2$. No gráfico identificado por B torna-se visível que o mapeamento do espaço de entrada para espaço de característica cria uma melhor distribuição dos dados para traçar o hiperplano.

\figura[h]{img/svm.pdf}{}{svm}{width=1\linewidth}%


Entretanto, calcular o mapeamento dos dados para maiores dimensões pode ser extremamente custoso em termos computacionais. A complexidade de tal transformação é quadrática, $O(n^2)$, tornando o algoritmo pouco eficiente para dados de grandes dimensões \cite{hamel2011knowledge}. Como solução pra esse problema \textcite{boser1992training} apresentam o truque do \textit{kernel}, uma técnica que faz uso de funções \textit{kernel} que permitem descobrir o produto interno entre dois vetores em dimensões maiores, sem a necessidade de utilizar a função de mapeamento $\phi(x)$, para o espaço de características. Formalmente o \textit{kernel} é dado por $k(x_i, x_j)$, onde $x_i$ e $x2_j$ são vetores do espaço de entrada \cite{scholkopf2001learning}. Tal função é escolhida a \textit{priori} e define como a classificação se comportará, entre eles os mais comuns são: kernel linear dado pela \refEq{kernel1}, kernel polinomial definido pela \refEq{kernel2} e o kernel RBF gaussiano, \refEq{kernel3}.

\equacao{kernel1}{k(x_i, x_j) = x_j \cdot x_i}
\equacao{kernel2}{k(x_i, x_j) = (1+x_i \cdot x_j)^d}
\equacao{kernel3}{k(x_i, x_j) = exp(-\|x_i-x_j\|^2)}

% Tendo em vista a função de otimização da SVM adquirida através dos multiplicadores de Lagrange, representada pela \refEq{svm1}, e a igualdade estabelecida pelo truque do kernel, $k(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$, o produto de mapeamento pode ser substituído pela função \textit{kernel} gerando a \refEq{svm2}, uma nova função de otimização com complexidade linear.

% \begin{align}
% \label{phieq}
% \phi(x) &= \begin{bmatrix}
%       x_1 x_1 \\
%       x_2 x_1 \\
%       x_2 x_2 \\
%       \vdots \\
%       x_m x_m
%      \end{bmatrix}
% \end{align}

% \equacao{svm1}{max_{\alpha \in \R}W(\alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i, j=1}^{m} y_i y_j \alpha_i \alpha_j \color{red} (\phi(x_i) \cdot \phi(x_j) )}

% \equacao{svm2}{max_{\alpha \in \R}W(\alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i, j=1}^{m} y_i y_j \alpha_i \alpha_j \color{red}k(x_i, x_j)}

Entendido o básico do algoritmo da SVM, resta agora definir a função de decisão entre duas classes, representadas pelos rótulos $-1$ e $1$. A definição se dá pela \refEq{svmdec}, onde $x$ é o vetor de entrada, $x_i$ é o iésimo vetor de suporte, $y = \pm 1$ é o rótulo padrão, $b$ é o parâmetro inicial do hiperplano, $\alpha$ é o iésimo multiplicador de Lagrange para o hiperplano ótimo, e por último $k(x, x_i)$ é a função kernel.

\equacao{svmdec}{f(x) = sgn\Bigg (\sum_{i=1}^{m} y_i \alpha_i k(x,x_i) + b \Bigg)}


%Do meu ponto de vista, o melhor livro de deep learning são os livros de redes neurais e depois estudar algum material específico pra lstm, dropout e etc.. %Esse livro é bom para ter uma noção, mas isso vai demandar esforço: Redes Neurais: Princípios e Prática (Simon Haykin)
\section{Aprendizado Profundo} 

A solução de diversas classes de problemas vem do entendimento e da experiência da adversidade em termos de uma hierarquia de conceitos. Cada conceito é definido em uma relação com conceitos mais simples, permitindo assim se aprender conceitos complexos \cite{deep_learning}. Esta abordagem de transformar conceitos complexos em simples, formando assim profundas camadas conceituais hierárquicas, é característica do aprendizado profundo \cite{deep_learning}. 

O problema central de representação do conhecimento, é resolvido no aprendizado profundo com a introdução de representações expressas em termos de outras mais simples \cite{deep_learning}. Seu exemplo tipico é o Perceptron Multicamadas, uma função matemática que mapeia um conjunto de valores de entrada para valores de saída. Tal função é formada de diversas outras funções mais simples, e pode-se pensar nessas funções como novas representações do valor de entrada. Considerando um exemplo prático, a \refFig{deepdog} representa um perceptron multicamadas para classificar uma imagem em cachorro, gato ou humano. 

\figura[h]{img/mlp.pdf}{Perceptron multicamadas classificando uma imagem de cachorro. Cada cor de camada escondida representa uma conceito da imagem sendo observada pelo modelo}{deepdog}{width=.8\linewidth}%

Na \refFig{deepdog}, a função é mapear uma imagem em uma das três categorias apresentadas. O aprendizado profundo realiza essa tarefa quebrando a imagem complexa e avaliando mapeamentos aninhados mais simples. Cada um desses mapeamentos é denominado de camada. A primeira camada é a de entrada, ou camada visível, é nesta camada que os dados são fornecidos ao modelo. As camadas subsequentes são denominadas camadas escondidas, cada uma extraindo conceitos cada vez mais abstratos da entrada (no nosso exemplo, da imagem). Estas camadas são denominadas escondidas, porque seu valor não é fornecido pelo dado de entrada, cabendo ao modelo determina-los. Após todas as camadas calcularem seus valores, a camada de saída vai ativar o resultado do que esta sendo avaliado. 

Este processo de ativação acontece devido a um mecanismo denominado função de ativação. Tal função determina quando uma camada vai ativar ou não cada neurônio da rede, dependendo da importância dele para avaliação da entrada atual. Elas são computacionalmente eficientes, e ajudam a normalizar a saída em um intervalo de $-1$ a $1$ ou $0$ a $1$. Em aplicações modernas de aprendizagem profunda, essas funções assumem formas não lineares e suas equações variam dependendo da aplicação. 

Dentro dos diversos formatos de dados que um modelo pode realizar uma análise, dados experimentais que foram observados em diferentes momentos no tempo abordam uma dificuldade a mais e trazem dificuldades únicas \cite{shumway2017time}. Estes dados são denominados de séries temporais ou sequências, e a abordagem na qual se estuda as questões matemáticas e estatísticas das correlações temporais é comumente referida como análise de séries temporais \cite{shumway2017time}. Esta abordagem se concentra na modelagem de algum valor futuro de uma série como uma função paramétrica dos valores atuais e passados, gerando resultados no domínio do tempo como uma ferramenta de previsão \cite{shumway2017time}. Alguns exemplos de problemas que podem ser estudados com séries temporais são: aquecimento global, reconhecimento de texto e áudio, condições climáticas, e a bolsa de valores. 

O estudo destas séries pode ser abordado utilizando técnicas de aprendizado profundo. No entanto, o estudo de séries necessita um conceito chave de aprendizado de máquina: a possibilidade de compartilhar parâmetros entre diferentes partes do modelo \cite{deep_learning}. Em abordagens com parâmetros não compartilhados, não conseguiria-se generalizar sequências que não foram observadas durante o processo de aprendizagem para cada valor de tempo~\cite{deep_learning}. Quando um parâmetro é compartilhado, torna-se possível a extensão e aplicação de um modelo em dados de diferentes formas, e realizar generalizações através deles~ \cite{deep_learning}. Tal compartilhamento é particularmente importante quando um pedaço especifico de informação pode acontecer em múltiplas posições dentro de uma sequência. Do ponto de vista de aprendizagem de máquina, essa técnica é aplicada com o auxilio de redes recorrentes e recursivas, particularmente com uma classe de redes neurais para processar dados sequenciais denominada \acrlong{RNN}.

%comentário geral
\subsection{Redes Neurais Recorrentes}

O processo de aprendizado, seja humano ou de máquina, necessita da capacidade de processar dados de forma sequencial. Sem este processamento, o ser humano não seria capaz de entender um texto nem assistir um filme sem perder o contexto da situação \cite{}. Redes neurais tradicionais, como o Perceptron, não são capazes de processar sequências de dados \cite{}. No entanto, com a ideia do compartilhamentos de parâmetros, a classe de \acrfull{RNNs} \cite{rnn} busca resolver esse problema. A \refFig{rnn} ilustra uma estrutura básica de um dos modelo mais simples da classe.

\figura[h]{img/rnn.pdf}{Topologia de uma unidade \acrshort{RNN} simples}{rnn}{width=.5\linewidth}%

Uma \acrshort{RNN} possui um mecanismo de ciclo, que permite a comunicação de informações de um passo de tempo $t$ para o próximo passo $t+1$. Essas informações são denominadas de estados escondidos $h_t$, e representam a entrada dos dados ao longo do tempo. Na \refFig{rnn}, o modelo observa uma entrada $x_t$ e calcula uma saída $h_t$, e com o ciclo presente passa a informação para o próximo passo da rede onde $h_t$ se torna $h_{t-1}$. Assim, a \acrshort{RNN} da \refFig{rnn} pode ser pensada como múltiplas cópias da mesma rede, cada qual comunicando uma mensagem para seu sucessor.

No entanto, as \acrshort{RNN}s simples possuem grandes dificuldades para aprender sequências de informações muito longas \cite{}. Tal dificuldade acontece porque as \acrshort{RNNs} possuem um valor denominado gradiente, que é responsável pela permanência de informação. Quando muita informação é alimentada no modelo, o gradiente começa a diminuir de forma significativa, causando com que as informações prévias percam valor, esta limitação é denominada de dissipação do gradiente \cite{}. Entender as dependências de longo prazo ainda é um dos maiores desafios em aprendizado profundo \cite{deep_learning}.

Até a escrita deste trabalho, a principal forma de tratamento para a limitação da dissipação do gradiente foi a ideia de incluir caminhos pelo tempo que possuem valores de gradiente que não desaparecem, nem crescem de forma descontrolada \cite{deep_learning}. Modelos baseados nesse mecanismo são denominados de \acrshort{RNNs} baseada em portões, e são capazes de decidir que informações desejam guardar ou esquecer para melhorar o desempenho do processo de decisão. Estas redes incluem a \acrlong{LSTM} e o \acrlong{GRU}, apresentados nas Subseções \ref{sec:lstm} e \ref{sec:gru}.

\subsection{Long Short-Term Memory}
\label{sec:lstm}

Uma rede \acrfull{LSTM}~\cite{lstm} tem como característica uma arquitetura de \acrshort{RNN} baseada em portões, que pode ser aplicada à previsão de dados temporais \cite{}. Além de ser o modelo de sequência mais eficaz, a LSTM mostrou resultados substanciais ao explorar inter-correlações de longo prazo~\cite{lstm, deep_learning}. O uso de um modelo baseado em portões supera a dificuldade da \acrshort{RNN} simples em aprender dependências de longo prazo com mais de alguns intervalos de tempo \cite{lstm}, adereçando o problema da série temporal com uma abordagem viável.

\figura[h]{img/lstm.pdf}{Topologia de uma unidade LSTM}{lstm}{width=.8\linewidth}%

A \refFig{lstm} apresenta uma topologia para uma célula LSTM. A célula é composta de um estado $c_t$, que transmite informação através da cadeia, implicando em uma redução dos efeitos da memória de curto prazo; e portões, mecanismos internos que regulam o fluxo de informação. Os mecanismos de portões (veja novamente na \refFig{lstm}) permitem informações serem adicionadas ou removidas no estado da célula através do processo de aprendizado por funções $sigmoide$. Estas funções são representadas pela \refEq{sigmoid}, onde $\mathnormal{e}$ representa o número de Euler, e $z$ o número real a ser escalado.

\equacao{sigmoid}{g(z) = \frac{1}{1+\mathnormal{e}^{-z}}}

A estrutura da LSTM possui três portões principais, cada qual com uma tarefa especifica, como visto na \refFig{lstm}. O portão do esquecimento é o que decide a permanência de informações. Valores de estados prévios $c_{t-1}$ e valores de entrada atual $x_t$ são passados para a função $sigmoide$, gerando um fator de esquecimento. O portão de entrada é responsável por atualizar o estado da célula com informações da entrada atual $x_t$ e de estados prévios $c_{t-1}$. Esses valores são passados para a função $sigmoide$ que decide quais informações serão atualizadas, e posteriormente para a função $tangente$ para regular a rede. A saída da função de ativação $sigmoide$ decide quais informações manter da função tangente, por uma operação de multiplicação. Por último, o portão de saída decide qual deve ser o valor do próximo estado oculto $h_t$. Este estado contém informações de entradas prévias e é responsável pela predição do modelo. Seu valor é calculado com o estado oculto prévio $h_{t-1}$ e com a entrada atual $x_t$ em uma função $sigmoide$ que calcula o valor $out_t$. Este valor $out_t$ é posteriormente multiplicado pela $tangente$ do estado atual $c_t$, para decidir que informação o novo estado oculto $h_t$ deve conter.

Após a nova entrada $x_t$ ser processada nos portões, o valor do estado da célula é atualizado. Seu valor é obtido primeiramente multiplicando o estado prévio $c_{t-1}$ pelo fator de esquecimento, possivelmente removendo informações desnecessárias, e então realizando uma operação de adição com o valor resultante do portão de entrada, atualizando assim o valor de estado $c_{t-1}$ para um novo valor $c_{t}$. O novo valor $c_t$ e o valor oculto $h_t$ são propagados pela cadeia para o próximo passo até que todos os passos sejam realizados, e assim o modelo realiza o aprendizado profundo.

Como entrada a célula LSTM recebe um vetor de três dimensões com a forma: $x_t = \langle {batch\textrm{ }size, passos, características} \rangle$. O \emph{batch size} define o número de amostas que vão ser propagadas pela rede; os $passos$ representam qual o período de tempo de cada uma das amostras são; e as $características$ são o número de dimensões que são alimentadas a cada $passo$, no qual cada dimensão é um dado de aprendizado. Como um exemplo, uma aplicação no mercado de ações poderia considerar um $passo$ sendo um dia útil de mercado. Com isso, se a entrada contem $12$ $passos$ com $32$ de $batch$ $size$ e o preço de fechamento sua única $característica$, uma entrada teria o formato de $32$ amostas de preço de fechamento de cada um dos $12$ dias.

Uma camada LSTM também pode considerar múltiplas unidades. Quando esta configuração acontece, a LSTM consistirá de $n$ cópias independentes dela mesmo, cada cópia contendo uma estrutura idêntica, mas inicializada com pesos diferentes, e portanto, computando diferentemente. Com isso, ao usar $n$ unidades, a camada LSTM vai produzir $n$ saídas. 

\subsection{Gated Recurrent Unit}
\label{sec:gru}

Outro tipo de \acrshort{RNN} baseada em portões é a \acrfull{GRU} \cite{gru}, que foi motivada pela \acrshort{LSTM}, mas sua implementação e computação são muito mais simples \cite{gru}. Com base no conhecimento da LSTM, pode-se analisar a GRU de forma similar. Sua topologia, como vista na \refFig{gru}, remove o estado da célula $c_t$ e usa um estado escondido $h_t$ para transferir informações no seu lugar. Além disso a GRU contém outros portões, o de redefinição e o de atualização.

\figura[h]{gru.pdf}{Topologia de uma unidade GRU}{gru}{width=.7\linewidth}%

A \refFig{gru} apresenta uma topologia para uma célula \acrshort{GRU}. O portão de atualização funciona de forma similar a ambos os portões de esquecimento e de entrada de uma \acrshort{LSTM}, cabe a ele decidir quais informações são uteis \cite{deep_learning}. Logo, o portão é responsável por decidir quanto de informação é passado do estado oculto $h_t$ atual para o próximo $h_{t+1}$ \cite{gru}. Já o portão de redefinição é utilizado para decidir quanto de informação passada vai ser esquecida, introduzindo um efeito adicional não linear na relação entre estados passados e futuros \cite{deep_learning}. Quando o portão de redefinição chega próximo a zero, o estado oculto $h_t$ é forçado a esquecer informações prévias e redefinir-se com apenas a entrada atual $x_t$ \cite{gru}. Portanto, o portão permite que estados ocultos esqueçam informações irrelevantes, permitindo uma representação mais compacta dos dados \cite{gru}.

Assim como a \acrshort{LSTM}, a \acrshort{GRU} tem sua entrada $x_t$ do mesmo formato, e também pode considerar múltiplas unidades, cada unidade computando diferentemente. Como cada unidade \acrshort{GRU} possui portões de atualização e redefinição independentes, cada unidade aprenderá a capturar informações em diferentes escalas de tempo \cite{gru}. Aquelas unidades que aprenderam informações de curto prazo, tenderão a ter seus portões de redefinição frequentemente ativos. Por outro lado, as unidades que aprenderam informações de longo prazo, vão ter seus portões de atualização mais ativos. 


\section{Aprendizado Por Reforço}
\label{sec:rl}

Situações em que uma entidade não conhece nada sobre o ambiente inserido e precisa descobrir o que fazer são características do Aprendizado por Reforço. Nestes ambientes a entidade aprendiz precisa descobrir como mapear situações com ações, de forma a maximizar um sinal de recompensa numérico \cite{rl_intro}. A entidade aprendiz, não sabe quais ações realizar, nem como o ambiente funciona, assim precisando explorar o custo de suas ações como forma de aprendizado \cite{modern_approach}. Esta forma de aprendizado pode ser comparada com muitos ambientes não computacionais. Por exemplo, um bebê aprendendo a andar, aprende a se equilibrar e a se movimentar com seus próprios erros, tomando ações exploratórias para conseguir ficar mais tempo em pé e começar a andar.

As principais características do aprendizado por reforço são o agente e o ambiente \cite{machinelearning}. A \refFig{reforco} demonstra a interação, em um espaço de tempo (ou passo), entre ambos. O ambiente é o meio em que o agente esta inserido, podendo ser modificado pelo agente, ou sofrer mudanças por si só \cite{rl_intro}. O agente por sua vez, interage com o ambiente em cada passo da simulação, realizando uma ação com base em sua observação feita do estado do ambiente \cite{rl_intro}. O agente recebe também do ambiente uma realimentação numérica do impacto de suas ações em cada passo, chamada de recompensa ou reforço \cite{modern_approach}. O objetivo do agente é maximizar sua recompensa acumulada durante a simulação, denominada retorno. Métodos de aprendizagem por reforço são meios de um agente aprender comportamentos para alcançar um objetivo.

\figura[h]{img/reforco.pdf}{Interação básica agente-ambiente no aprendizado por reforço}{reforco}{width=.8\linewidth}%

O aprendizado por reforço possui conceitos intrínsecos que são necessárias para entender o seu funcionamento, sendo os principais:

\begin{itemize}
    \item \textbf{Estado e Observação:} Um estado $s$ é a descrição completa do ambiente, não existe informação sobre o ambiente que não é refletida pelo estado \cite{rl_intro}. Quando a informação não reflete o ambiente de forma completa, descrevendo-o de forma parcial, essa informação é chamada de observação \cite{machinelearning}. Quando o agente consegue observar o estado completo do ambiente, o ambiente é denominado totalmente observável. Por outro lado, quando apenas uma parte é percebida, denominamos o ambiente de parcialmente observável.
    
    \item \textbf{Espaço de Ações:} Ambientes diferentes permitem diferentes tipos de ações $a$. O conjunto de todas as ações validas em dado ambiente é denominado espaço de ações \cite{rl_intro}. Alguns ambientes, possuem espaço de ação discretos, onde um numero finito de movimentos está disponível para o agente. Outros ambientes, como ambientes robóticos, possuem espaço de ações contínuos, onde ações são vetores de valores reais. Esta distinção implica consequências para métodos em aprendizagem por reforço profunda \cite{SpinningUp2018}. Alguns algoritmos podem ser aplicados somente em um dos casos, e precisaria de uma remodelagem substancial para trocar \cite{SpinningUp2018}. As ações são realizadas por um agente de acordo com sua política.

    \item \textbf{Política:} Pode-se pensar na política como o cérebro de um agente racional, uma política é uma regra usada por um agente para decidir quais ações tomar \cite{machinelearning}. Ela pode ser determinística, denotado na \refEq{poldet} por $\mu$ ou pode ser estocástica, denotada na \refEq{polest} por $\pi$.

    \equacao{poldet}{a_t = \mu(s_t)}
    \equacao{polest}{a_t \sim \pi(\cdot | s_t)}

    Em aprendizado por reforço profunda, políticas são parametrizadas \cite{deeprl}. Logo, a saída de uma política parametrizada são funções dependentes de um conjunto de parâmetros ajustáveis por algum algoritmo de otimização \cite{deeprl}. Neste trabalho, parâmetros de tais políticas serão representados por $\theta$ ou $\phi$ de forma subscrita no simbolo da política:

    \equacao{polparm}{
        a_t = \mu_{\theta}(s_t) \\
        a_t \sim \pi_{\theta}(\cdot | s_t)
    }
    
    \item \textbf{Trajetória:} Uma trajetória, ou episódio, $\tau$ é uma sequência de estados e ações no ambiente.

    \equacao{trajetoria}{\tau = (s_0, a_0, s_1, a_1, ...)}

    Transições de estados, são mudanças que acontece no ambiente entre o estado atual $s_t$ e o estado futuro $s_{t+1}$ \cite{rl_intro}. Tais transições são regidas por leis naturais do ambiente, e dependem apenas da ação mais recente $a_t$, podendo ser deterministas $s_{t+1} = f(s_t, a_t)$ ou estocásticas $s_{t+1} \sim P(\cdot|s_t, a_t)$ \cite{rl_intro}.

    \item \textbf{Recompensa:} A função de recompensa, como mencionada anteriormente, é um conceito de extrema importância para o aprendizado por reforço. Seu valor $r_t$ é descrito pela \refEq{recompensa} e depende do estado atual do ambiente $s_t$, da ação tomada $a_t$ e do próximo estado do meio $s_{t+1}$ \cite{rl_intro}.

    \equacao{recompensa}{r_t = R(s_t, a_t, s_{t+1})}

    O objetivo do agente é maximizar o valor acumulado da recompensa ao longo de uma trajetória \cite{rl_intro}. Este valor é denominado retorno $R(\tau)$, mas pode representar noções diferentes dependendo da aplicação. Uma das possíveis formas do retorno, representado pela \refEq{nondiscrew}, é o não descontado de horizonte finito, a soma de todos as recompensas obtidas $r_t$ em um intervalo de tempo $T$ \cite{deeprl}. A outra abordagem é remover o intervalo de tempo $T$ e realizar uma soma de todas as recompensas obtidas, descontadas em quanto tempo no futuro cada uma foi obtida \cite{rl_intro}. Esta abordagem é dominada retorno descontado de horizonte infinito, representada pela \refEq{discrew}, e requer um termo adicional denominado fator de desconto, denotado por $\gamma \in (0,1)$ \cite{machinelearning}.

    \equacao{nondiscrew}{R(\tau) = \sum_{t=0}^T r_t}
    \equacao{discrew}{R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t}

    A necessidade de descontar uma recompensa em vez de recebe-las por completo é visado por dois motivos. Primeiro que, a recompensa imediata é geralmente mais atraente que recompensas futuras \cite{SpinningUp2018}. E segundo, que matematicamente, uma soma de horizonte infinita de recompensas pode não convergir para um valor finito, criando dificuldades matemáticas. Com o fator de desconto e sob circunstancias sensatas a soma infinita converge \cite{SpinningUp2018}.
    
    \item \textbf{Funções de Valor:} Estas funções são um conceito fundamental em aprendizado por reforço e são usadas, em quase todos os algoritmos \cite{deeprl}. Elas são responsáveis por conhecer o valor de um estado (ou par estado-ação), onde valor é a métrica de retorno esperado se você iniciar em um determinado estado (ou par estado-ação) e agir de acordo com uma política específica para sempre \cite{rl_intro}.

    Estas funções obedecem equações de auto-consistência denominadas Equações de Bellman \cite{bellman, rl_intro}. A ideia básica por trás destas equações é: O valor do seu ponto de partida é a recompensa que você espera receber por estar lá, mais o valor de onde quer que você for em seguida \cite{bellman}.
    
\end{itemize}


\subsection{Gradiente de Política Determinística Profunda}

%falar do ator e do critico, referenciar figura

No aprendizado por reforço profundo, o algoritmo do \acrfull{DDPG} \cite{silver2014deterministic, ddpg}, combina ideias dos recentes avanços do aprendizado profunda e do aprendizagem por reforço, resultando em um algoritmo que resolve de maneira robusta problemas desafiadores em domínios com espaços de ação contínuos \cite{ddpg}. 

\figura[h]{img/ddpg.pdf}{Diagrama de funcionamento do DDPG}{ddpg}{width=\linewidth}

O \acrshort{DDPG} é um algoritmo que aprende simultaneamente uma função ação-valor $Q$ e uma política, graças a um mecanismo de redes neurais profundas denominado de ator-critico. A \refFig{ddpg} apresenta um diagrama de seu funcionamento. O ator (cor azul na \refFig{ddpg}) é o responsável por utilizar dados fora da política e uma equação de Bellman para aprender a função $Q$, e posteriormente o crítico (cor verde) utiliza a função $Q$ aprendida pra aprender sua política. Sua função ação-valor $Q$ é baseada em um algoritmo clássico de aprendizagem por reforço, o \emph{Q-Learning}, e é motivado da mesma forma: se a função ação-valor ótima $Q^*(s,a)$ é conhecida, então em qualquer estado, a ação ótima $a^*(s)$ pode ser encontrada resolvendo a \refEq{eitcha}.

\equacao{eitcha}{a^*(s) = \arg \max_a Q^*(s,a)}

Portanto, o algoritmo do \acrshort{DDPG} alterna entre aprender o um aproximador para $Q^*(s,a)$ (ator) e aprender um aproximador para $a^*(s)$ (crítico), e realiza essa tarefa de forma especificamente adaptada para ambientes de com espaço de ações contínuos.

De forma matemática, o \acrshort{DDPG} se adapta em ambientes contínuos de acordo com a forma que computa o valor máximo das suas ações em $\max_a Q^*(s,a)$. Descobrir este valor máximo não seria uma tarefa problemática em espaço de ações discretos, quando se existe números finitos de ações pode-se comparar todos os valores diretamente um com o outro. No entanto, ao adaptar o problema para espaços contínuos, avaliar de forma bruta todos os espaços possíveis e resolver o problema de otimização, significaria calcular o máximo de infinitos valores em todo passo da simulação. Este custo temporal não é aceitável para algoritmos de aprendizado. Logo, por o espaço de ação ser contínuo, a função $Q^*(s,a)$ é assumida ser derivável em relação a ação. Isso permite estabelecer uma regra de aprendizado eficiente, baseada em gradiente, para uma política $\mu(s)$ que explora esse fato. Assim, em vez de resolver uma tarefa custosa quando $\max_a Q(s,a)$ precisa ser calculado, realiza-se uma aproximação do valor com $\max_a Q(s,a) \approx Q(s,\mu(s))$.

A matemática básica por traz do funcionamento do \acrshort{DDPG} é apresentada a seguir em duas partes: aprender a função Q, e aprender uma política.

\subsubsection{Aprendizado da Função Q}

No algoritmo do \acrshort{DDPG} existe uma equação de Bellman que descreve a função de ação-valor ótima $Q^*(s,a)$. Esta equação é descrita pela \refEq{funcaovalor}, onde $s' \sim P$ significa que o próximo estado $s'$, é uma amostra do ambiente dado pela distribuição normal $P(\cdot| s,a)$.

\equacao{funcaovalor}{Q^*(s,a) = \underset{s' \sim P}{{\mathrm E}}\left[r(s,a) + \gamma \max_{a'} Q^*(s', a')\right]}

Esta equação de Bellman é o ponto de partida para aprender um ator para aproximar $Q^*(s,a)$. Esse ator será descrito como uma rede neural $Q_{\phi}(s,a)$, com parâmetros $\phi$. Com o propósito de avaliar o desempenho desse ator, precisa-se construir uma função de perda que indique o quanto $Q_{\phi}$ chega a satisfazer \refEq{funcaovalor}. Essa função é denominada \acrfull{EQMB} (representada pela \refEq{eqmb}), e pressupõe a existência de um conjunto de ${\mathcal D}$ de transições $(s,a,r,s',d)$ passadas (onde d indica se $s'$ é um estado final). 

\equacao{eqmb}{
    L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a') \right) \Bigg)^2
    \right]
}

Algoritmos de \emph{Q-Learning} para aproximação de funções, como o \acrshort{DDPG}, são baseados na minimização da função \acrshort{EQMB}. Para tal minimização, a implementação do algoritmo neste trabalho conta com duas técnicas apresentadas na \refFig{ddpg}:

\begin{itemize}
    \item \textbf{Replay Buffer:} Um mecanismo de armazenamento do conjunto ${\mathcal D}$ de experiencias prévias. Para o algoritmo, ter um comportamento estável, o conjunto tem que ser grande o suficiente para conter um grande números de experiencias, mas não deve ser grande a ponto de conter tudo. Se apenas for usado dados muito recentes, o modelo vai realizar um sobreajuste e não irá funcionar; se muitas experiencias forem usadas o processo de aprendizado vai ficar muito custoso.
    
    \item \textbf{Redes Neurais Alvo:} Esta técnica utiliza o termo alvo descrito na \refEq{alvo} com o objetivo de aproximar a função $Q$ deste valor. Realizando assim, a minimização da função \acrshort{EQMB}.

    \equacao{alvo}{r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a')}
    
    No entanto, o problema de minimização da \acrshort{EQMB} é instável, devido ao fato do alvo depender dos mesmos parâmetros $\phi$ que estamos tentando treinar. Para adereçar esse problema é necessário a utilização de um conjunto de parâmetros que se aproximam de $\phi$, mas com um atraso de tempo. Este atraso é implementado com a utilização de redes neurais adicionais, denominadas redes neurais alvo, uma para o ator e outra para o crítico, com o objetivo de atrasar suas redes locais respectivas (observe novamente a \refFig{ddpg}). Os parâmetros destas segundas redes são denominados $\phi_{\text{targ}}$, e as redes são atualizadas a cada atualização da sua rede primaria respectiva.
\end{itemize}

Portanto, juntando ambas as técnicas, o aprendizado da função $Q$ no \acrshort{DDPG} é realizado minimizando a \refEq{emqbfinal} com gradiente descendente estocástico, onde $\mu_{\theta_{\text{targ}}}$ é a política alvo.

\equacao{emqbfinal}{
L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) Q_{\phi_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s')) \right) \Bigg)^2
    \right]
}

\subsubsection{Aprendizado de Política}

Após o aprendizado da função $Q$, o aprendizado da política é relativamente simples. O algoritmo precisa aprender uma política determinística $\mu_{\theta}(s)$, que retorna a ação que maximiza o valor $Q_{\phi}(s,a)$. Assim, pelo espaço de ações ser continuo, e pela suposição de que a função $Q$ é derivável em relação a ação, pode-se realizar um cálculo de gradiente acendente, apenas com os parâmetros de política, para resolver a \refEq{politica}.

\equacao{politica}{
\max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\phi}(s, \mu_{\theta}(s)) \right]
}

O algoritmo do \acrshort{DDPG} treina uma política determinística de com dados fora da política. Devido, a política ser determinística, se o agente decidisse explorar somente com dados da política, no começo provavelmente não teria uma variedade de ações ampla o suficiente para o agente receber sinais de recompensa significativos. Assim, para o algoritmo explorar melhor, o algoritmo adiciona um ruído nas ações do agente enquanto treina, e posteriormente, na fase de teste, esse ruído é retirado para avaliação significativa da qualidade em que a política aproveita o que aprendeu.