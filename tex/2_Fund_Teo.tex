\label{cap:fund}

Neste capítulo, apresentam-se os conceitos básicos de economia e mercado, discorrendo sobre o mercado de ações e a teoria moderna de portfólio. Em seguida, discutem-se conceitos relacionados a área de aprendizado de máquina, apresentando juntamente algoritmos relevantes para este trabalho. Posteriormente é apresentado o conceito de aprendizado profundo e seus algoritmos utilizados. Por fim, discorre-se sobre o aprendizado por reforço com técnicas de aprendizado profundo.

\section{Economia e Mercado}

A riqueza material de uma sociedade é determinada pela capacidade produtiva de sua economia, representada pelos bens e serviços que fornece aos membros da mesma~\cite{investments}. Essa capacidade produtiva, ou produtividade, é definida diretamente pelos chamados ativos reais, tais como construções, conhecimento, terras, máquinas e qualquer elemento da sociedade que são necessários para gerar esses bens~\cite{investments}.

Em contraste aos ativos reais existem os ativos financeiros. Tais ativos não possuem a capacidade de representar a riqueza de uma sociedade, contudo conseguem afetar a produtividade de maneira indireta~\cite{investments}. Ações e títulos são os exemplos mais simples de ativos financeiros. Esses conseguem definir os donos de propriedades e frações de empresas, gerando assim uma facilidade na transação de recursos entre os proprietários de um ativo real~\cite{investments}.

Em suma, ativos reais produzem bens e serviços, enquanto ativos financeiros definem alocação de riqueza e lucro entre investidores~\cite{investments}. No entanto, ativos financeiros representam posses de frações de ativos reais. Como exemplo, quando o dinheiro é investido em uma firma, ele é convertido em ativos reais. Se tal investimento oferecer um retorno, ele pode ser retribuído em ativos financeiros através de títulos que representam parte da empresa. Os títulos, consequentemente, representam frações de ativos reais. Portanto, ativos financeiros e os mercados nos quais são negociados preenchem um papel importante no desenvolvimento da economia~\cite{investments}.

\subsection{O Mercado de Ações}

Assim como as instituições financeiras e seguradoras surgiram naturalmente a partir das necessidades de investidores, o mercado financeiro não é diferente~\cite{investments}. A necessidade de uma instituição para unir, em um mesmo lugar, interessados em negociar ativos financeiros fez surgir os mercados de ações e as bolsas de valores \cite{investments}.

Chama-se de mercado toda interação entre dois ou mais interessados que se identifiquem como compradores e vendedores. Dentre os tipos de mercado podemos dividi-los em quatro categorias: (i) o mercado de pesquisa direta, em que os compradores e vendedores precisam se procurar por conta própria; (ii) o mercado intermediado, no qual existe um intermediador, entre o comprador e o vendedor, que obterá algum lucro na negociação; (iii) o mercado de revendedores em que o intermediador compra os ativos e revende para os vendedores; e por último (iv) o mercado de leilão, no qual todos os transacionistas se reúnem em um mesmo local para fazerem ofertas \cite{investments}. O mercado de leilão é o melhor exemplo de mercado financeiro moderno, criando a vantagem de reunir todas as ofertas em um mesmo lugar sem a necessidade de pesquisas por preços mais baratos \cite{investments}.

Dentre os mercados de leilão, existem os mercados de ações, comumente chamados de bolsa de valores. Em geral, a bolsa de valores é um mercado no qual são efetuadas ações de compra e venda de valores mobiliários (ações, títulos, derivativos, e entre outros) \cite{gomes1997bolsa}. Tais valores podem ser vistos como contratos legais que representam o direito de receber um benefício futuro~\cite{elton2012moderna}. Neste trabalho destacaremos dois principais valores mobiliários: ações preferenciais e ações ordinárias \cite{elton2012moderna}. Ações ordinárias representam o direito de um investidor sobre os rendimentos ou ativos de uma corporação, possuindo como principal característica a responsabilidade limitada de seus detentores \cite{investments}. Como por exemplo, se a corporação for a falência o máximo que o investidor de ações ordinárias pode perder é o seu investimento inicial, não tendo acesso aos ativos gerais da empresa. Já as ações preferenciais são semelhantes com as ações ordinários exceto pela sua prioridade nos pagamentos dos dividendos\footnote{Parte dos lucros de uma empresa que são distribuídos aos seus acionistas como forma de remuneração.} da empresa. Caso a empresa vá a falência todos os acionistas com ações preferenciais devem receber os dividendos antes dos pagamentos de dividendos de ações ordinárias \cite{elton2012moderna}.

As ações, tanto ordinárias como preferenciais, possuem suas cotas negociadas na bolsa de valores seguindo uma mecânica bem definida, como apresentada na \refFig{bolsa}. Um indivíduo que deseja comprar ou vender uma ação necessita antes contratar uma corretora, uma entidade que possui permissão para emitir ordens em uma bolsa de valores. Uma ordem é responsável por anexar as informações de compra ou venda de uma ação. Nas informações da ordem há: nome do negociante e qual ativo deseja negociar, se é uma compra ou uma venda, a quantidade de ativos a serem negociados e o prazo que a ordem permanecerá válida. Assim que a ordem é emitida e executada a corretora é responsável por transacionar o dinheiro entre o indivíduo e a bolsa de valores \cite{elton2012moderna}, como demonstrado na \refFig{bolsa}.


\figura[htbp]{img/bolsa.pdf}{Diagrama do processo de compra e venda de ações na bolsa de valores}{bolsa}{width=.7\linewidth}%

\subsection{Teoria Moderna do Portfólio}

Um indivíduo que possui riquezas, pode ter seus bens referidos como um conjunto de ativos que possui, seja eles reais ou financeiros. Por exemplo, um investidor que possui imóveis e títulos do tesouro direto, os imóveis representam seus ativos reais e os títulos seus ativos financeiros, juntos correspondem a um total de riqueza. Esse conjunto também pode ser chamado de portfólio ou carteira~\cite{elton2012moderna}. A composição dos portfólios é resultada de decisões casuais ou podem ser fim de um planejamento pré-estabelecido~\cite{elton2012moderna}.

A criação de portfólios planejados com premissa nos conceitos de otimização e diversificação da carteira, foram pilares no desenvolvimento e entendimento dos mercados financeiros \cite{kolm201460}. Em 1952 \textcite{markowitz}, publicou o artigo \emph{Portfolio Selection} que viria a ser um dos maiores avanços na criação de portfólios~\cite{kolm201460}. Sua proposta ficou conhecida como a Teoria Moderna do Portfólio, ou \acrfull{VMM} \cite{markowitz}. A teoria busca responder à questão fundamental de como um investidor deve alocar seus fundos dentre as diversas possibilidades de escolha de investimentos. 

Markowitz adereçou essa questão com duas etapas. Primeiro ele quantificou o retorno e risco de um valor mobiliário \cite{kolm201460}. Depois, propôs que os investidores considerassem o retorno e risco de forma conjunta, determinando a alocação dos fundos com base no balanceamento entre os fatores \cite{kolm201460}. Portanto, os fatores devem ser calculados em relação ao portfólio como um todo, e não somente ao ativo especifico. O retorno do portfólio $R_p$ foi quantificado de acordo com a medida estatística de retorno esperado demonstrada na \refEq{retorno}, onde $R_i$ é o retorno do ativo $i$, e $w_i$ a proporção do ativo $i$ na carteira \cite{elton2012moderna}. Enquanto o risco da carteira $\displaystyle \varphi_{p}$ foi medido pelo seu desvio padrão, demonstrado na \refEq{desviop}, onde $\varphi_{i}^{2}$ é a variância do retorno do ativo $i$, $\varphi_{j}^{2}$ a variância do retorno do ativo $j$, e $\rho_{ij}$ o coeficiente de correlação entre os ativos $i$ e $j$ \cite{elton2012moderna}. O coeficiente $\rho$ é limitado entre $[-1,1]$, onde $1$ significa que os dois ativos vão se mover sempre em equilíbrio, enquanto o valor $-1$ significa que seus movimentos são exatamente opostos.

\equacao{retorno}{\operatorname {E} (R_{p})=\sum _{i}w_{i}\operatorname {E} (R_{i})\quad} 

\equacao{desviop}{\displaystyle \varphi_{p}=\sqrt{\sum _{i}w_{i}^{2}\varphi _{i}^{2}+\sum _{i}\sum _{j\neq i}w_{i}w_{j}\varphi _{i}\varphi _{j}\rho _{ij}}}

O entendimento de que o processo de tomada de decisão financeira sólida é realizado como um balanceamento entre retorno e risco foi revolucionária \cite{kolm201460}. O princípio mais inovador foi o da diversificação do portfólio \cite{kolm201460}. Com esse princípio, o risco de um portfólio pode passar a ser analisado de acordo com a correlação dos seus constituintes, e não apenas o risco do ativo independentemente. Esse conceito era estranho à análise financeira clássica, que girava em torno da crença de que os investidores deveriam investir naqueles ativos que oferecem o maior valor futuro, dado seu preço atual \cite{kolm201460}. A ideia de Markowitz, foi inovadora também por passar a formular o problema de tomadas de decisões financeiras, em um problema de otimização \cite{kolm201460}. Em particular, o modelo de otimização da \acrshort{VMM} propõe que entre os números infinitos de portfólio que alcançam um retorno pré-definido, o investidor deve escolher aquele que tem o menor risco.

\figura[htbp]{img/fronteiraeficiente.pdf}{Fronteira Eficiente de um conjunto de ativos. Adaptado de~\cite{elton2012moderna}}{fronteira}{width=.45\linewidth}

O problema de otimização da carteira pode ser visto com uma interpretação geométrica das combinações de ativos \cite{elton2012moderna}. Ao realizar a análise, pode-se desenhar o subconjunto de carteiras que serão as preferidas de todos os investidores com aversão a risco \cite{elton2012moderna}. Tal conjunto, denominado de fronteira eficiente, é o envoltório externo de todas as possíveis carteiras, entre a carteira de menor risco (ou de variância mínima global) e a carteira de máximo retorno \cite{elton2012moderna}. A \refFig{fronteira} ilustra a interpretação geométrica da otimização da carteira, e a fronteira eficiente. Cabe então ao investidor decidir, dado o balanceamento entre retorno e risco que ele deseja assumir, em que ponto da fronteira seu portfólio será montado. 


\section{Aprendizado de Máquina} 

Durante milhares de anos os seres humanos vêm procurando desvendar como funcionam seus pensamentos, como um punhado de matéria pode perceber, compreender, prever e manipular um mundo muito maior que ele mesmo \cite{modern_approach}. O campo da \acrfull{IA} estende o desejo de desvendar e entender o pensamento humano, indo além, construindo entidades inteligentes \cite{modern_approach}. O desejo de construir tais entidades capazes de pensar vem desde a Grécia antiga, que apresenta diversos inventores e vidas artificiais em sua mitologia \cite{deep_learning}. Até o momento de escrita deste trabalho a \acrshort{IA} vem se mostrando uma subárea da ciência da computação próspera, com diversas aplicações práticas e tópicos de pesquisa \cite{deep_learning, ga_optimized_lstm, random_forest_macroeconomic, airms, clustering_svm, hybrid_forecasting, starcraft, nn_forecasting}. Algumas de suas aplicações são realizações de tarefas, tais como: jogos eletrônicos \cite{starcraft}, demonstração de teoremas matemáticos \cite{bibel2013automated}, criação de música \cite{dhariwal2020jukebox}, controle robótico \cite{gu2017deep}, e diagnóstico de doenças \cite{coronga}.

Pode-se definir a \acrshort{IA} de diversas maneiras: em termos de processos de pensamento e raciocínio, comportamento, fidelidade ao desempenho humano, ou racionalidade \cite{modern_approach}. Para o escopo deste trabalho, a \acrshort{IA} foi conceituada como o processo de agir com racionalidade, definido por \textcite{poole1998computational}, ``Inteligência computacional é o estudo do projeto de agentes inteligentes''. Segundo \textcite{modern_approach}, um agente é tudo que pode ser considerado capaz de perceber seu ambiente por meio de sensores e de agir sobre o ambiente por meio de atuadores. Para um agente humano, os olhos, ouvidos e outros sentidos são os sensores que percebem o mundo, já suas pernas, mãos bocas e outras partes do corpo são atuadores que interagem com ele. Um agente é dito inteligente, ou racional, quando realizam as melhores ações para maximizar alguma medida de desempenho pré-definida para o problema sendo tratado \cite{modern_approach}. Como exemplo, considere um agente investindo na bolsa de valores, sua medida de desempenho pode ser considerada o lucro ganho com seus investimentos. Esse agente é considerado racional caso a ação realizada possa vir a maximizar o seu lucro.

Em diversos cenários, como o exemplo citado, um comportamento considerado bom em determinado momento pode deixar de ser em um outro contexto ou situação. Sendo assim, um agente racional necessita aprender a se adaptar em situações adversas \cite{modern_approach}. Dizemos que o agente aprende, quando melhora o seu desempenho nas tarefas futuras após observar informações atuais disponíveis a ele \cite{modern_approach}. Ainda, no contexto de agente inteligente, o aprendizado apresenta duas vantagens, sendo elas: (i) um programador não consegue antecipar todas as situações possíveis que um agente pode encontrar; e (ii) as vezes o programador não tem ideia de como resolver o problema, e deixa a tarefa de descoberta para o agente \cite{modern_approach}.

Diante do exposto anteriormente, os algoritmos de \acrfull{ML} surgem como um meio para auxiliar o agente a adquirir o próprio conhecimento para aprender \cite{machinelearning}. O surgimento de algoritmos de \acrshort{ML} permitiu computadores abordarem problemas envolvendo conhecimento do mundo real e tomar decisões que pareçam subjetivas \cite{deep_learning}. De maneira direta, aprender é o processo de converter experiências em conhecimento. A entrada de um algoritmo de aprendizado 
é um conjunto de dados de treinamento, representando a experiência, e a saída é um conhecimento sobre a tarefa que está sendo aprendida \cite{shalev2014understanding}.

Em \acrshort{ML}, existem diversos tipos de tarefas para a entidade aprendiz. Essas tarefas podem ser agrupadas de acordo com o paradigma de aprendizado para lidar com ela, podendo ser preditiva ou descritiva \cite{faceli2011inteligencia}. Uma tarefa preditiva busca encontrar uma função que faz o mapeamento dos dados de entrada para obter uma predição como saída \cite{modern_approach}. Algoritmos que seguem o modelo preditivo são do paradigma de aprendizado supervisionado. O termo supervisionado, se relaciona com a necessidade de existir um ``supervisor externo'', também conhecido como especialista, que conhece a resposta da tarefa (rótulo), para analisar o desempenho do aprendiz \cite{faceli2011inteligencia}. Por outro lado, em uma tarefa descritiva o objetivo é explorar ou descrever um conjunto de dados. Algoritmos que se encaixam nesse tipo de classificação de tarefa são do paradigma de aprendizado não supervisionado \cite{faceli2011inteligencia}. Esses algoritmos não são fornecidos um rótulo \cite{modern_approach} e, portanto, não necessitam de uma supervisão externa. Outro paradigma de aprendizado, é o de aprendizado por reforço. Em tal paradigma a meta é reforçar ou recompensar de forma positiva ou negativa uma ação realizada pela entidade que está aprendendo \cite{faceli2011inteligencia}. Essa categoria de aprendizado será apresentada na Subseção \ref{sec:rl}.

Tem-se como escopo deste trabalho resolver tarefas de predição, logo discorre-se majoritariamente sobre o aprendizado supervisionado. Um algoritmo supervisionado é uma função que dado um conjunto de dados de entrada rotulado, constrói um estimador~\cite{faceli2011inteligencia}. Os rótulos tomam valores em um domínio conhecido. Se o domínio conter valores nominais, subclassificamos a tarefa como classificação~\cite{modern_approach}. Caso o contrário, os rótulos possuírem um conjunto infinito e ordenado de valores tem-se um problema de regressão~\cite{faceli2011inteligencia}. Com um exemplo prático, uma predição na bolsa de valores pode se tomar de ambas as formas. Se deseja-se descobrir se uma determinada ação vai ganhar ou perder valor, temos um problema de classificação. Se deseja descobrir uma estimativa do preço de uma ação, o problema toma a forma de regressão, porque o domínio do valor da ação é de qualquer valor real positivo. Com o objetivo de realizar uma tarefa de predição, diversos algoritmos de aprendizado supervisionado foram criados. Para este trabalho, foram explorados os algoritmos \acrlong{KNN} e \acrlong{SVM} que são discutidos nas Subseções \ref{sec:knn} e \ref{sec:svm}.

\subsection{k-Vizinhos Mais Próximos}
\label{sec:knn}

No aprendizado supervisionado, a simplicidade do \acrfull{KNN} \cite{knn} se destaca entre os modelos, sendo um dos algoritmos mais usados no problema de classificação \cite{larose2014discovering}. O \acrshort{KNN} classifica objetos desconhecidos com base na similaridade com outros objetos já conhecidos~\cite{larose2014discovering}. Tais objetos são definidos pelos seus atributos e representados como pontos em um espaço, nos quais suas similaridades são comumente definidas como a distância euclidiana entre os pontos \cite{larose2014discovering}. Portanto, quando o algoritmo recebe uma instância não conhecida $x_t$, o \acrshort{KNN} calcula a distância entre $x_t$ e todos os outros pontos presentes no espaço. O rótulo selecionado para $x_t$ será o mesmo do objeto $x_i$ onde $x_i$ possui a menor distância euclidiana até $x_t$ \cite{faceli2011inteligencia}.

O \acrshort{KNN} não precisa necessariamente definir a similaridade com a distância euclidiana~\cite{larose2014discovering}. Outras abordagens podem apresentar melhor desempenho alterando a forma de medir essa similaridade. Como por exemplo, a \acrfull{DTW} \cite{dtw}, utilizada principalmente pela sua eficiência como séries temporais pela sua capacidade de mover e distorcer os dados a fim de facilitar o encontro de padrões similares \cite{senin2008dynamic}. Tendo isso em vista pode-se definir qualquer métrica de similaridade que seja uma função real $\delta$ de modo que para quaisquer coordenadas $x$, $y$ e $z$ as propriedades definidas pelas \refEqs{prop1}{prop3} se satisfazem \cite{larose2014discovering}.

\equacao{prop1}{\delta(x,y) >= 0\\ \delta(x,y) = 0 \iff x = y}
\equacao{prop2}{\delta(x,y) = \delta(y,x)}
\equacao{prop3}{\delta(x,z) <= \delta(x,y) + \delta(y,z)}

A propriedade da \refEq{prop1} garante que a distância sempre será não negativa e que a única maneira de ser zero é caso as coordenadas sejam as mesmas. Já a propriedade da \refEq{prop2} indica que a distância de $x$ para $y$ deve ser a mesma de $y$ para $x$. Por último, a \refEq{prop3} representa a desigualdade triangular, que define que a introdução de um terceiro ponto não pode reduzir a distância entre outros dois pontos.

Salienta-se, entretanto, que o \acrshort{KNN} considera $k$ vizinhos próximos como base para rotular a instância nunca vista, gerando uma votação entre os vizinhos mais próximos para decidir o rótulo da entrada teste \cite{faceli2011inteligencia}. Considere a \refFig{knn} como exemplo, dado um valor de entrada $x_t$ o algoritmo seleciona os $k$ vizinhos (no exemplo, $k=5$)  mais próximos de $x_t$. Com isso, cada vizinho vota em uma classe e $x_t$ é classificado segundo a classe mais votada. Tal votação pode ser formalmente definida pela \refEq{voto1}, em que a função de moda retorna o elemento que mais se repete, representando a classe mais votada.

\figura[htbp]{img/knn.pdf}{Classificação do elemento $x_t$ através do algoritmo \acrshort{KNN}}{knn}{width=.7\linewidth}%

O método de votação para classificar uma entrada não se restringe somente a moda, o \acrshort{KNN} também pode ser usado em problemas de regressão, o que traz duas outras maneiras de se abordar a classe mais votada, em relação a função de perda que se deseja minimizar \cite{faceli2011inteligencia}. Caso a função a ser minimizada seja o erro quadrático o valor do rótulo de $x_t$ torna-se a média dos $k$ vizinhos mais próximos, formalmente dado pela \refEq{voto2} \cite{larose2014discovering}. Para o caso de que a função a ser minimizada é o desvio padrão utiliza-se a mediana como métrica de votação, sendo formalizada pela \refEq{voto3}.

% Encontrar símbolos usados na estatística para moda, média e mediana
\equacao{voto1}{f'(x_t) = moda(f(x_1), f(x_2),..., f(x_k))}
\equacao{voto2}{f'(x_t) = m\acute{e}dia(f(x_1), f(x_2), \ldots, f(x_k))}
\equacao{voto3}{f'(x_t) = mediana(f(x_1), f(x_2), \ldots, f(x_k))}

\subsection{Máquina de Vetores de Suporte}
\label{sec:svm}

Uma \acrfull{SVM} \cite{boser1992training} é um modelo de aprendizado supervisionado que mapeia pontos de um espaço de entrada, para um espaço de características. O \acrshort{SVM} realiza o aprendizado gerando um plano para separar classes de dados e realizar a tomada de decisões, denominado hiperplano separador \cite{scholkopf2001learning}. A ideia central, é separar classes de dados utilizando o hiperplano separador, de tal forma que os elementos de classes opostas, mais próximos entre si, fiquem o mais distante possível dos planos gerados~\cite{hamel2011knowledge}.

Dependo da dimensão dos dados de entrada o hiperplano irá assumir uma dimensão a menos do que a dimensão dos dados. Por exemplo, em dados com duas dimensões o hiperplano separador terá uma única dimensão, sendo representado por uma linha. A lógica se mantém para dimensões superiores, se o espaço for tridimensional o hiperplano separador será um plano de duas dimensões \cite{hamel2011knowledge}. Entretanto, não é sempre que o problema apresenta dados linearmente separáveis sendo necessário mapear o espaço de entrada para dimensões superiores a fim de encontrar melhores hiperplanos.

Na \refFig{svm}, é mostrado o processo de encontro do hiperplano ótimo para uma entrada de dados que não é linearmente separável. Os dados são mapeados para um espaço de características de maior dimensão através da função $z = x^2 + y^2$. Na \refFig{svm}, torna-se visível que o mapeamento do espaço de entrada para espaço de característica cria uma melhor distribuição dos dados para traçar o hiperplano.

\figura[htbp]{img/svm.pdf}{Separação das classes num espaço de características de maior dimensão utilizando um hiperplano separador}{svm}{width=1\linewidth}%

Entretanto, calcular o mapeamento dos dados para maiores dimensões pode ser extremamente custoso em termos computacionais. A complexidade de tal transformação é quadrática, $O(n^2)$, tornando o algoritmo pouco eficiente para dados de grandes dimensões \cite{hamel2011knowledge}. Como solução para essa limitação, \textcite{boser1992training} apresentam o ``truque'' do \textit{kernel}, uma técnica que faz uso de funções \textit{kernel} que permitem descobrir o produto interno entre dois vetores em dimensões maiores, sem a necessidade de utilizar a função de mapeamento $\phi(x)$, para o espaço de características. Formalmente o \textit{kernel} é dado por $\kappa(x_i, x_j)$, onde $x_i$ e $x_j$ são vetores do espaço de entrada \cite{scholkopf2001learning}. Tal função é escolhida a \textit{priori} e define como a classificação se comportará, entre eles os mais comuns são: (i) kernel linear, \refEq{kernel1}; (ii) kernel polinomial \refEq{kernel2}, onde $d$ é a dimensão da entrada; e (iii) kernel RBF gaussiano, \refEq{kernel3}.

\equacao{kernel1}{\kappa(x_i, x_j) = x_j \cdot x_i}
\equacao{kernel2}{\kappa(x_i, x_j) = (1+x_i \cdot x_j)^d}
\equacao{kernel3}{\kappa(x_i, x_j) = \mathnormal{e}^{(-\|x_i-x_j\|^2)}}

A \acrshort{SVM} utiliza uma função chamada função de decisão para efetuar a classificação, entre os rótulos $-1$ e $1$, de dados não rotulados. A definição da função se dá pela \refEq{svmdec}, onde $m$ é o tamanho do vetor de entrada, $x_t$ é o vetor de entrada, $x_i$ é o i-ésimo vetor de suporte, $y = \pm 1$ é o rótulo padrão, $b$ é o parâmetro inicial do hiperplano, $\lambda_i$ é o i-ésimo multiplicador de Lagrange para o hiperplano ótimo, $\kappa(x_t, x_i)$ é a função kernel e a função $sinal$ retorna $-1$ para valores abaixo de $0$, $1$ para valores maiores que $0$ e zero para o valor $0$.

\equacao{svmdec}{f(x_t) = sinal\Bigg (\sum_{i=1}^{m} y_i \lambda_i \kappa(x_t,x_i) + b \Bigg)}

\section{Aprendizado Profundo} 

A solução de diversas classes de problemas vem do entendimento e da experiência da adversidade em termos de uma hierarquia de conceitos. Cada conceito é definido em uma relação com conceitos mais simples, permitindo assim se aprender conceitos complexos~\cite{redesneurais}. Essa abordagem de transformar conceitos complexos em simples, formando assim profundas camadas conceituais hierárquicas, é característica do aprendizado profundo~\cite{deep_learning}. 

O problema central de representação do conhecimento, é resolvido no aprendizado profundo com a introdução de representações expressas em termos de outras mais simples \cite{redesneurais}. Seu exemplo típico é o Perceptron Multicamadas, uma função matemática que mapeia um conjunto de valores de entrada para valores de saída. Tal função é formada por diversas outras funções mais simples, podendo se pensar como novas representações do valor de entrada. Considerando um exemplo prático, a \refFig{deepdog} representa um perceptron multicamadas para classificar uma imagem em cachorro, gato ou humano. 

\figura[htbp]{img/mlp.pdf}{Perceptron multicamadas classificando uma imagem de cachorro. Cada cor de camada escondida representa um conceito da imagem sendo observada pelo modelo. Adaptado de~\cite{deep_learning}}{deepdog}{width=.8\linewidth}%

Na \refFig{deepdog}, o objetivo é mapear uma imagem em uma das três categorias apresentadas. O aprendizado profundo realiza essa tarefa quebrando a imagem complexa e avaliando mapeamentos aninhados mais simples. Cada um dos mapeamentos é denominado de camada. A primeira camada é a de entrada, ou camada visível, é nela que os dados são fornecidos ao modelo. As camadas subsequentes são denominadas camadas escondidas, cada uma extraindo conceitos cada vez mais abstratos da entrada (no exemplo proposto, da imagem). Essas camadas são denominadas escondidas, porque seu valor não é fornecido pelo dado de entrada, sendo responsabilidade do modelo determina-las. Após todas as camadas calcularem seus valores, a camada de saída vai ativar o resultado do que está sendo avaliado. 

Esse processo de ativação acontece devido a um mecanismo denominado função de ativação. Tais funções são responsáveis por determinar quando uma camada vai ativar ou não cada elemento da rede, dependendo da importância dele para avaliação da entrada atual. As funções de ativação são computacionalmente eficientes, e ajudam a normalizar a saída em um intervalo de $-1$ a $1$ ou $0$ a $1$. Em aplicações modernas de aprendizagem profunda, as funções de ativação assumem formas não lineares e suas equações variam dependendo da aplicação.

O aprendizado profundo, auxilia a resolver problemas de diversas áreas, e de diversos formatos de dados. Uma das principais áreas abordada pelo aprendizado profundo, é o estudo de dados que foram observados em diferentes momentos no tempo \cite{shumway2017time}. A análises de séries temporais estudam a matemática e estatística das correlações temporais dos dados, se concentrando na modelagem de algum valor futuro como uma função paramétrica dos valores atuais e passados \cite{shumway2017time}. Gera-se então, resultados no domínio do tempo como uma ferramenta de previsão \cite{shumway2017time}. Alguns exemplos de problemas que podem ser estudados com séries temporais são: aquecimento global \cite{rolnick2019tackling}, reconhecimento de texto \cite{radford2019language}, e a bolsa de valores \cite{forecasting_returns}.

No entanto, o estudo de séries necessita de um conceito chave de aprendizado de máquina: a possibilidade de compartilhar parâmetros entre diferentes partes do modelo \cite{deep_learning}. Em abordagens com parâmetros não compartilhados, não conseguir-se-ia generalizar sequências que não foram observadas durante o processo de aprendizagem para cada valor de tempo~\cite{deep_learning}. Quando um parâmetro é compartilhado, torna-se possível a extensão e aplicação de um modelo em dados de diferentes formas, e realizar generalizações através deles~\cite{deep_learning}. Tal compartilhamento é particularmente importante quando um pedaço específico de informação pode acontecer em múltiplas posições dentro de uma sequência. Do ponto de vista de aprendizagem de máquina, a abordagem do compartilhamento de parâmetros é aplicada com o auxílio de redes recorrentes e recursivas. Particularmente com uma classe das redes neurais para processar dados sequenciais denominada \acrlong{RNN}.

\subsection{Redes Neurais Recorrentes}

O processo de aprendizado, seja humano ou de máquina, necessita da capacidade de processar dados de forma sequencial. Sem esse processamento, o ser humano não seria capaz de entender um texto nem assistir um filme sem perder o contexto da situação \cite{deep_learning}. Redes neurais tradicionais, como o Perceptron, não são capazes de processar sequências de dados \cite{redesneurais}. No entanto, com a ideia dos compartilhamentos de parâmetros, a classe das Redes Neurais Recorrentes (RNNs) \cite{rnn} busca resolver essa limitação. A \refFig{rnn} ilustra uma estrutura básica de um dos modelos mais simples da classe, a topologia de uma unidade \acrshort{RNN}.

\figura[htbp]{img/rnn.pdf}{Topologia de uma unidade \acrshort{RNN} simples}{rnn}{width=.5\linewidth}%

Uma \acrshort{RNN} possui um mecanismo de ciclo, que permite a comunicação de informações de um passo de tempo $t$ para o próximo passo $t+1$. Essas informações são denominadas de estados escondidos $h_t$, e representam a entrada dos dados ao longo do tempo. Na \refFig{rnn}, o modelo observa uma entrada $x_t$ e calcula uma saída $h_t$, e com o ciclo presente passa a informação para o próximo passo da rede, no qual $h_t$ se torna $h_{t-1}$. Assim, a \acrshort{RNN} da \refFig{rnn} pode ser pensada como múltiplas cópias da mesma rede, cada qual comunicando uma mensagem para seu sucessor.

No entanto, as \acrshort{RNN}s simples possuem dificuldades para aprender sequências de informações longas \cite{lstm}. Tal dificuldade acontece porque as \acrshort{RNN}s possuem um valor denominado gradiente, que é responsável pela permanência da informação. Quando muita informação é alimentada no modelo, o gradiente começa a diminuir de forma significativa, causando com que as informações prévias percam valor. Essa limitação é denominada de dissipação do gradiente \cite{redesneurais}. Entender as dependências de longo prazo ainda é um dos maiores desafios em aprendizado profundo \cite{deep_learning}.

Até a escrita deste trabalho, a principal forma de tratamento para a limitação da dissipação do gradiente foi a ideia de incluir caminhos pelo tempo que possuem valores de gradiente que não desaparecem, nem crescem de forma descontrolada \cite{deep_learning}. Modelos baseados nesse mecanismo são denominados de \acrshort{RNN}s baseada em portões, e são capazes de decidir que informações desejam guardar ou esquecer para melhorar o desempenho do processo de decisão. Essas redes incluem a \acrlong{LSTM} e o \acrlong{GRU}, apresentados nas Subseções \ref{sec:lstm} e \ref{sec:gru}.

\subsection{Long Short-Term Memory}
\label{sec:lstm}

Uma rede \acrfull{LSTM}~\cite{lstm} tem como característica uma arquitetura de \acrshort{RNN} baseada em portões. Sendo possível ser aplicada à previsão de dados temporais \cite{deep_learning}. Além de ser o modelo de sequência mais eficaz, a LSTM mostrou resultados substanciais ao explorar intercorrelações de longo prazo~\cite{lstm, deep_learning}. O uso de um modelo baseado em portões supera a dificuldade da \acrshort{RNN} simples em aprender dependências de longo prazo com mais de alguns intervalos de tempo \cite{lstm}, adereçando o problema da série temporal com uma abordagem viável.

\figura[htbp]{img/lstm.pdf}{Topologia de uma unidade LSTM}{lstm}{width=.8\linewidth}%

A \refFig{lstm} apresenta uma topologia para uma célula \acrshort{LSTM}. A célula é composta de um estado $c_t$, que transmite informação através da cadeia, implicando em uma redução dos efeitos da memória de curto prazo; e portões, mecanismos internos que regulam o fluxo de informação. Os mecanismos de portões (veja novamente na \refFig{lstm}) permitem informações serem adicionadas ou removidas no estado da célula através do processo de aprendizado por funções $sigmoide$. Essas funções são representadas pela \refEq{sigmoid}, onde $\mathnormal{e}$ representa o número de Euler, e $x$ o número real a ser escalado.

\equacao{sigmoid}{\sigma(x) = \frac{1}{1+\mathnormal{e}^{-x}}}

A estrutura da \acrshort{LSTM} possui três portões principais, cada qual com uma tarefa especifica, como visto na \refFig{lstm}. O portão do esquecimento é o que decide a permanência de informações. Valores de estados prévios $c_{t-1}$ e valores de entrada atual $x_t$ são passados para a função $sigmoide$, gerando um fator de esquecimento. O portão de entrada é responsável por atualizar o estado da célula com informações da entrada atual $x_t$ e de estados prévios $c_{t-1}$. Esses valores são passados para a função $sigmoide$ que decide quais informações serão atualizadas, e posteriormente para a função $tangente$ para regular a rede. A saída da função de ativação $sigmoide$ decide quais informações manter da função tangente, por uma operação de multiplicação. Por último, o portão de saída decide qual deve ser o valor do próximo estado oculto $h_t$. Esse estado contém informações de entradas prévias e é responsável pela predição do modelo. Seu valor é calculado com o estado oculto prévio $h_{t-1}$ e com a entrada atual $x_t$ em uma função $sigmoide$ que calcula o valor $out_t$. O valor $out_t$ é posteriormente multiplicado pela $tangente$ do estado atual $c_t$, para decidir que informação o novo estado oculto $h_t$ deve conter.

Após a nova entrada $x_t$ ser processada nos portões, o valor do estado da célula é atualizado. Seu valor é obtido primeiramente multiplicando o estado prévio $c_{t-1}$ pelo fator de esquecimento, removendo informações desnecessárias. Posteriormente, é realizada uma operação de adição com o valor resultante do portão de entrada, atualizando assim o valor de estado $c_{t-1}$ para um novo valor $c_{t}$. O novo valor $c_t$ e o valor oculto $h_t$ são propagados pela cadeia para o próximo passo até que todos os passos sejam realizados, e assim o modelo realiza o aprendizado profundo.

Como entrada a célula LSTM recebe um vetor de três dimensões com a forma: $x_t = \langle {batch\textrm{ }size, passos, caracter\acute{\imath}sticas} \rangle$. O \emph{batch size} define o número de amostras que vão ser propagadas pela rede; os $passos$ representam qual o período de tempo de cada uma das amostras são; e as $caracter\acute{\imath}sticas$ são o número de dimensões que são alimentadas a cada $passo$, no qual cada dimensão é um dado de aprendizado. Como um exemplo, uma aplicação no mercado de ações poderia considerar um $passo$ sendo um dia útil de mercado. Com isso, se a entrada contem $12$ $passos$ com $32$ de $batch$ $size$ e o preço de fechamento sua única $caracter\acute{\imath}istica$, uma entrada teria o formato de $32$ amostras de preço de fechamento de cada um dos $12$ dias.

Uma camada LSTM também pode considerar múltiplas unidades. Quando tal configuração acontece, a LSTM consistirá de $n$ cópias independentes dela mesmo, cada cópia contendo uma estrutura idêntica, mas inicializada com pesos diferentes e, portanto, computando diferentemente. Com isso, ao usar $n$ unidades, a camada LSTM vai produzir $n$ saídas. 

\subsection{Gated Recurrent Unit}
\label{sec:gru}

Outro tipo de \acrshort{RNN} baseada em portões é a \acrfull{GRU} \cite{gru}. A \acrshort{GRU} foi motivada pela \acrshort{LSTM}, mas sua implementação e computação são muito mais simples \cite{gru}. Com base no conhecimento da \acrshort{LSTM}, pode-se analisar a \acrshort{GRU} de forma similar. Sua topologia, como vista na \refFig{gru}, remove o estado da célula $c_t$ e usa um estado escondido $h_t$ para transferir informações no seu lugar. Além disso a \acrshort{GRU} contém outros portões, o de redefinição e o de atualização, como apresentado na \refFig{gru}.

\figura[htbp]{gru.pdf}{Topologia de uma unidade GRU}{gru}{width=.7\linewidth}%

O portão de atualização funciona de forma similar aos portões de esquecimento e de entrada de uma \acrshort{LSTM}, cabe a ele decidir quais informações são uteis~\cite{deep_learning}. Logo, o portão é responsável por decidir quanto de informação é passado do estado oculto $h_t$ atual para o próximo $h_{t+1}$ \cite{gru}. Já o portão de redefinição é utilizado para decidir quanto de informação passada vai ser esquecida, introduzindo um efeito adicional não linear na relação entre estados passados e futuros \cite{deep_learning}. Quando o portão de redefinição chega próximo a zero, o estado oculto $h_t$ é forçado a esquecer informações prévias e redefinir-se com apenas a entrada atual $x_t$ \cite{gru}. Portanto, o portão permite que estados ocultos esqueçam informações irrelevantes, permitindo uma representação mais compacta dos dados \cite{gru}.

Assim como a \acrshort{LSTM}, a \acrshort{GRU} tem sua entrada $x_t$ do mesmo formato, e também pode considerar múltiplas unidades, cada unidade computando diferentemente. Como cada unidade \acrshort{GRU} possui portões de atualização e redefinição independentes, cada unidade aprenderá a capturar informações em diferentes escalas de tempo \cite{gru}. Aquelas unidades que aprenderam informações de curto prazo, tenderão a ter seus portões de redefinição frequentemente ativos. Por outro lado, as unidades que aprenderam informações de longo prazo, vão ter seus portões de atualização mais ativos. 


\section{Aprendizado Por Reforço}
\label{sec:rl}

Situações em que uma entidade não conhece nada sobre o ambiente inserido e precisa descobrir o que fazer são características do \acrfull{RL}. Nos ambientes a entidade aprendiz precisa descobrir como mapear situações com ações, de forma a maximizar um sinal de recompensa numérico \cite{rl_intro}. A entidade aprendiz, não sabe quais ações realizar, nem como o ambiente funciona, assim precisando explorar o custo de suas ações como forma de aprendizado \cite{modern_approach}. Essa forma de aprendizado pode ser comparada com muitos ambientes não computacionais. Por exemplo, um bebê aprendendo a andar, aprende a se equilibrar e a se movimentar com seus próprios erros, tomando ações exploratórias para conseguir ficar mais tempo em pé e começar a andar.

As principais características do \acrshort{RL} são o agente e o ambiente \cite{machinelearning}. A \refFig{reforco} apresenta a interação, em um espaço de tempo (ou passo), entre ambos. O ambiente é o meio em que o agente está inserido, podendo ser modificado pelo agente, ou sofrer mudanças por si só \cite{rl_intro}. O agente por sua vez, interage com o ambiente em cada passo da simulação, realizando uma ação com base em sua observação feita do estado do ambiente \cite{rl_intro}. O agente recebe também do ambiente uma realimentação numérica do impacto de suas ações em cada passo, chamada de recompensa ou reforço \cite{modern_approach}. O objetivo do agente é maximizar sua recompensa acumulada durante a simulação, denominada retorno. Métodos de \acrshort{RL} são meios de um agente aprender comportamentos para alcançar um objetivo.

\figura[htbp]{img/reforco.pdf}{Interação básica agente-ambiente no aprendizado por reforço}{reforco}{width=.8\linewidth}%

O \acrshort{RL} possui conceitos intrínsecos que são necessários para entender o seu funcionamento, sendo os principais:

\begin{itemize}
    \item \textbf{Estado e Observação:} Um estado $s$ é a descrição completa do ambiente, não existe informação sobre o ambiente que não é refletida pelo estado \cite{rl_intro}. Quando a informação não reflete o ambiente de forma completa, descrevendo-o de forma parcial, ela recebe o nome de observação \cite{machinelearning}. Quando o agente consegue observar o estado completo do ambiente, o ambiente é denominado totalmente observável. Por outro lado, quando apenas uma parte é percebida, denominamos o ambiente de parcialmente observável.
    
    \item \textbf{Espaço de Ações:} Ambientes diferentes permitem diferentes tipos de ações $a$. O conjunto de todas as ações validas em dado ambiente é denominado espaço de ações \cite{rl_intro}. Há ambientes que possuem espaço de ação discretos, nos quais um número finito de movimentos está disponível para o agente. Outros ambientes, como ambientes robóticos, possuem espaço de ações contínuos, nos quais ações são vetores de valores reais. A distinção do espaço de ações implica consequências para métodos em aprendizagem por reforço profunda \cite{SpinningUp2018}. Alguns algoritmos podem ser aplicados somente em um dos casos, e precisaria de uma remodelagem substancial para trocar \cite{SpinningUp2018}. As ações são realizadas por um agente de acordo com sua política.

    \item \textbf{Política:} Pode-se pensar na política como o cérebro de um agente racional. A política é uma regra usada por um agente para decidir quais ações realizar \cite{machinelearning}. Ela pode ser determinística, denotado na \refEq{poldet} por $\mu$ ou pode ser estocástica, denotada na \refEq{polest} por $\pi$.

    \equacao{poldet}{a_t = \mu(s_t)}
    \equacao{polest}{a_t \sim \pi(\cdot | s_t)}

    Em aprendizado por reforço profundo, políticas são parametrizadas \cite{deeprl}. Logo, a saída de uma política parametrizada são funções dependentes de um conjunto de parâmetros ajustáveis por algum algoritmo de otimização \cite{deeprl}. Neste trabalho, parâmetros de tais políticas serão representados por $\theta$ de forma subscrita no símbolo da política:

    \equacao{polparm}{
        a_t = \mu_{\theta}(s_t) \\
        a_t \sim \pi_{\theta}(\cdot | s_t)
    }
    
    \item \textbf{Trajetória:} Uma trajetória, ou episódio, $\tau$ é uma sequência de estados e ações no ambiente.

    \equacao{trajetoria}{\tau = (s_0, a_0, s_1, a_1, ...)}

    Transições de estados, são mudanças que acontecem no ambiente entre o estado atual $s_t$ e o estado futuro $s_{t+1}$ \cite{rl_intro}. Tais transições são regidas por leis naturais do ambiente, e dependem apenas da ação mais recente $a_t$, podendo ser determinísticas $s_{t+1} = f(s_t, a_t)$ ou estocásticas $s_{t+1} \sim P(\cdot|s_t, a_t)$ \cite{rl_intro}.

    \item \textbf{Recompensa:} A função de recompensa é um conceito de extrema importância para o aprendizado por reforço. Seu valor $r_t$ é descrito pela \refEq{recompensa} e depende do estado atual do ambiente $s_t$, da ação tomada $a_t$ e do próximo estado do meio $s_{t+1}$ \cite{rl_intro}.

    \equacao{recompensa}{r_t = R(s_t, a_t, s_{t+1})}

    O objetivo do agente é maximizar o valor acumulado da recompensa ao longo de uma trajetória \cite{rl_intro}. Esse valor é denominado retorno $R(\tau)$, mas pode representar noções diferentes dependendo da aplicação. Uma das possíveis formas do retorno, representado pela \refEq{nondiscrew}, é o não descontado de horizonte finito, a soma de todos as recompensas obtidas $r_t$ em um intervalo de tempo $T$ \cite{deeprl}. A outra abordagem é remover o intervalo de tempo $T$ e realizar uma soma de todas as recompensas obtidas, descontadas em quanto tempo no futuro cada uma foi obtida \cite{rl_intro}. Tal abordagem é dominada retorno descontado de horizonte infinito, representada pela \refEq{discrew}, e requer um termo adicional denominado fator de desconto, denotado por $\gamma \in (0,1)$ \cite{machinelearning}.

    \equacao{nondiscrew}{R(\tau) = \sum_{t=0}^T r_t}
    \equacao{discrew}{R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t}

    A necessidade de descontar uma recompensa em vez de recebe-las por completo é visado por dois motivos. Primeiro que, a recompensa imediata é geralmente mais atraente que recompensas futuras \cite{SpinningUp2018}. E segundo, que matematicamente, uma soma de horizonte infinita de recompensas pode não convergir para um valor finito, criando dificuldades matemáticas. Com o fator de desconto e sob circunstancias sensatas a soma infinita converge \cite{SpinningUp2018}.
    
    \item \textbf{Funções de valor:} As funções de valor são responsáveis por conhecer o valor de um estado (ou par estado-ação), no qual o valor é a métrica de retorno esperado se você iniciar em um determinado estado (ou par estado-ação) e agir de acordo com uma política específica para sempre \cite{rl_intro}. Tais funções obedecem a equações de auto consistência denominadas Equações de Bellman \cite{bellman, rl_intro}. A ideia básica por trás dessas equações é: O valor do seu ponto de partida é a recompensa que você espera receber por estar lá, mais o valor de onde quer que você for em seguida \cite{bellman}.
    
\end{itemize}


\subsection{Gradiente de Política Determinística Profunda}
\label{sec:ddpg}

No aprendizado por reforço profundo, o algoritmo do \acrfull{DDPG} \cite{silver2014deterministic, ddpg}, combina ideias dos recentes avanços do aprendizado profundo e do aprendizado por reforço, resultando em um algoritmo que resolve de maneira robusta problemas desafiadores em domínios com espaços de ação contínuos \cite{ddpg}. 

\figura[htbp]{img/ddpg.pdf}{Diagrama de funcionamento do DDPG. Adaptado de \cite{ddpg_diagram}}{ddpg}{width=0.9\linewidth}

O \acrshort{DDPG} é um algoritmo que aprende simultaneamente uma função ação-valor $Q$ e uma política, por meio de um mecanismo de redes neurais profundas denominado de ator-crítico. A \refFig{ddpg} apresenta um diagrama do funcionamento do \acrshort{DDPG}. O ator (cor azul na \refFig{ddpg}) é o responsável por utilizar dados fora da política e uma equação de Bellman para aprender a função $Q$. Posteriormente, o crítico (cor verde) utiliza a função $Q$ aprendida para aprender a política. A função ação-valor $Q$ é baseada em um algoritmo clássico de aprendizagem por reforço, o \emph{Q-Learning}, e é motivado da mesma forma: se a função ação-valor ótima $Q^*(s,a)$ é conhecida, então em qualquer estado, a ação ótima $a^*(s)$ pode ser encontrada resolvendo a \refEq{eitcha}.

\equacao{eitcha}{a^*(s) = \arg \max_a Q^*(s,a)}

Portanto, o algoritmo do \acrshort{DDPG} alterna entre aprender um aproximador para $Q^*(s,a)$ (ator) e aprender um aproximador para $a^*(s)$ (crítico), realizando a tarefa de forma especificamente adaptada para ambientes de com espaço de ações contínuos.

Matematicamente, o \acrshort{DDPG} se adapta em ambientes contínuos de acordo com a forma que computa o valor máximo das suas ações em $\max_a Q^*(s,a)$. Descobrir o valor máximo não seria uma tarefa problemática em espaço de ações discretos, quando se existe números finitos de ações pode-se comparar todos os valores diretamente um com o outro. No entanto, ao adaptar o problema para espaços contínuos, avaliar de forma bruta todos os espaços possíveis e resolver o problema de otimização, significaria calcular o máximo de infinitos valores em todo passo da simulação. Esse custo temporal não é aceitável para algoritmos de aprendizado. Logo, por o espaço de ação ser contínuo, a função $Q^*(s,a)$ é assumida ser derivável em relação a ação. Isso permite estabelecer uma regra de aprendizado eficiente, baseada em gradiente, para uma política $\mu(s)$ que explora esse fato. Assim, em vez de resolver uma tarefa custosa quando $\max_a Q(s,a)$ precisa ser calculado, realiza-se uma aproximação do valor com $\max_a Q(s,a) \approx Q(s,\mu(s))$.

A matemática básica por traz do funcionamento do \acrshort{DDPG} é apresentada a seguir em duas partes: aprender a função Q, e aprender uma política.

\subsubsection{Aprendizado da Função Q}

No algoritmo do \acrshort{DDPG} existe uma equação de Bellman que descreve a função de ação-valor ótima $Q^*(s,a)$. Essa equação é descrita pela \refEq{funcaovalor}, onde $s' \sim P$ significa que o próximo estado $s'$, é uma amostra do ambiente dado pela distribuição normal $P(\cdot| s,a)$.

\equacao{funcaovalor}{Q^*(s,a) = \underset{s' \sim P}{{\mathrm E}}\left[r(s,a) + \gamma \max_{a'} Q^*(s', a')\right]}

Essa equação de Bellman é o ponto de partida para aprender um ator para aproximar $Q^*(s,a)$. Esse ator será descrito como uma rede neural $Q_{\theta}(s,a)$, com parâmetros $\theta$. Com o propósito de avaliar o desempenho do ator, precisa-se construir uma função de perda que indique o quanto $Q_{\theta}$ chega a satisfazer a \refEq{funcaovalor}. Tal função é denominada \acrfull{EQMB} (representada pela \refEq{eqmb}), e pressupõe a existência de um conjunto de ${\mathcal D}$ de transições $(s,a,r,s',d)$ passadas (onde $d$ indica se $s'$ é um estado final). 

\equacao{eqmb}{
    L(\theta, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\theta}(s,a) - \left(r + \gamma (1 - d) \max_{a'} Q_{\theta}(s',a') \right) \Bigg)^2
    \right]
}

Algoritmos de \emph{Q-Learning} para aproximação de funções, como o \acrshort{DDPG}, são baseados na minimização da função \acrshort{EQMB}. Para tal minimização, a implementação do algoritmo neste trabalho conta com duas técnicas apresentadas na \refFig{ddpg}:

\begin{enumerate}
    \item \textbf{Replay Buffer:} Um mecanismo de armazenamento do conjunto ${\mathcal D}$ de experiências prévias. Para o algoritmo, ter um comportamento estável, o conjunto tem que ser grande o suficiente para conter uma quantidade relevante de experiências, mas não deve ser grande a ponto de conter tudo. Se apenas for usado dados muito recentes, o modelo vai realizar um sobreajuste e não irá funcionar; se muitas experiências forem usadas o processo de aprendizado vai ficar muito custoso.
    
    \item \textbf{Redes Neurais Alvo:} Uma técnica que utiliza o termo alvo descrito na \refEq{alvo} com o objetivo de aproximar a função $Q$ desse valor. Realizando assim, a minimização da função \acrshort{EQMB}.

    \equacao{alvo}{r + \gamma (1 - d) \max_{a'} Q_{\theta}(s',a')}
    
    No entanto, o problema de minimização da \acrshort{EQMB} é instável, devido ao fato de o alvo depender dos mesmos parâmetros $\theta$ que será treinado. Para adereçar esse problema é necessário a utilização de um conjunto de parâmetros que se aproximam de $\theta$, mas com um atraso de tempo. Para implementar o atraso, utiliza-se redes neurais adicionais, denominadas redes neurais alvo, uma para o ator e outra para o crítico, com o objetivo de atrasar suas redes locais respectivas (observe novamente a \refFig{ddpg}). Os parâmetros das redes secundárias são denominados $\theta_{\text{targ}}$, e as redes são atualizadas a cada atualização da sua rede primaria respectiva.
\end{enumerate}

Portanto, juntando ambas as técnicas, o aprendizado da função $Q$ no \acrshort{DDPG} é realizado minimizando a \refEq{emqbfinal} com gradiente descendente estocástico, onde $\mu_{\theta_{\text{targ}}}$ é a política alvo.

\equacao{emqbfinal}{
L(\theta, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\theta}(s,a) - \left(r + \gamma (1 - d) Q_{\theta_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s')) \right) \Bigg)^2
    \right]
}

\subsubsection{Aprendizado de Política}

Após o aprendizado da função $Q$, o algoritmo precisa aprender uma política determinística $\mu_{\theta}(s)$, que retorna à ação que maximiza o valor $Q_{\theta}(s,a)$. Assim, pelo espaço de ações ser continuo, e pela suposição de que a função $Q$ é derivável em relação a ação, pode-se realizar um cálculo de gradiente ascendente, apenas com os parâmetros de política, para resolver a \refEq{politica}.

\equacao{politica}{
    \max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\theta}(s, \mu_{\theta}(s)) \right]
}

Devido a política ser determinística, se o agente decidisse explorar somente com dados da política, no começo provavelmente não teria uma variedade de ações ampla o suficiente para o agente receber sinais de recompensa significativos. Portanto, o algoritmo do \acrshort{DDPG} treina uma política determinística com dados fora de sua política. Adicionalmente, para o melhorar a exploração, é necessário adicionar um ruído nas ações do agente enquanto treina. O ruído é retirado na fase de teste para avaliação significativa da qualidade em que a política aproveita o que aprendeu.